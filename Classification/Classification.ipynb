{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "import pandas as pd\n",
    "\n",
    "# Data preprocessing and trasformation (ETL)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, FunctionTransformer, Binarizer, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml, load_iris, make_moons, make_classification\n",
    "\n",
    "\n",
    "# Math and Stat modules\n",
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "from random import choice\n",
    "\n",
    "# Supervised Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, StratifiedKFold, RepeatedKFold, ShuffleSplit, StratifiedShuffleSplit, learning_curve, validation_curve\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "# Unsupervised Learning\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepariamo velocemente i dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_data = pd.read_csv('../ETLinSKLearn/BankChurnersMissingData.csv')\n",
    "credit_card_data.dropna(subset=['Total_Revolving_Bal','Months_Inactive_12_mon'],\n",
    "                   inplace=True\n",
    "                  )\n",
    "\n",
    "# Estraggo la  colonna delle label e la rimuovo dal dataset\n",
    "credit_card_label = credit_card_data['Attrition_Flag'].map(\n",
    "    {'Existing Customer':0,\n",
    "     'Attrited Customer':1\n",
    "    }\n",
    ").values\n",
    "credit_card_data.drop(columns=['Attrition_Flag',\n",
    "                               'CLIENTNUM',\n",
    "                               'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n",
    "                               'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1'],\n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_imputer(X, missing_value = 'Unknown'):\n",
    "    X = X.values\n",
    "    unique_values, count = np.unique(X,return_counts=True)\n",
    "    num_nan = count[unique_values == missing_value]\n",
    "    counting = count[unique_values != missing_value]\n",
    "    values = unique_values[unique_values != missing_value]\n",
    "    X_new = X.copy()\n",
    "    freq = counting / np.sum(counting)\n",
    "    X_new[X_new == missing_value] = np.random.choice(values,size=num_nan,p=freq)\n",
    "    return X_new\n",
    "\n",
    "ui = FunctionTransformer(unknown_imputer)\n",
    "\n",
    "customer_age_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "category_pipeline = Pipeline([\n",
    "    ('imputer', FunctionTransformer(unknown_imputer)),\n",
    "    ('ordinal', OneHotEncoder())\n",
    "])\n",
    "\n",
    "features_robust = ['Credit_Limit','Total_Revolving_Bal','Avg_Open_To_Buy']\n",
    "features_standard = list(set(credit_card_data.select_dtypes(include=['int64','float64']).columns).difference(set(features_robust + ['Avg_Utilization_Ratio', 'Customer_Age'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing = ColumnTransformer([\n",
    "    ('age', customer_age_pipeline, ['Customer_Age']),\n",
    "    ('gender', OrdinalEncoder(categories=[['M','F']]), ['Gender']),\n",
    "    ('edu', category_pipeline, ['Education_Level']),\n",
    "    ('status', category_pipeline, ['Marital_Status']),\n",
    "    ('income', category_pipeline, ['Income_Category']),\n",
    "    ('card', category_pipeline, ['Card_Category']),\n",
    "    ('numeric_robust', RobustScaler(), features_robust),\n",
    "    ('feature_standard', StandardScaler(), features_standard)\n",
    "],\n",
    "    remainder = 'passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = data_preprocessing.fit_transform(credit_card_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10125, 33)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['Customer_Age','Gender']\n",
    "for c in ['Education_Level','Marital_Status','Income_Category']:\n",
    "    cat_inc_name = [c+'_cat{}'.format(i) for i in range(1,len(credit_card_data[c].unique()))]\n",
    "    columns_name.extend(cat_inc_name)\n",
    "columns_name.extend(['Card_Category_cat{}'.format(i) for i in range(1,len(credit_card_data['Card_Category'].unique())+1)])\n",
    "columns_name.extend(features_robust)\n",
    "columns_name.extend(features_standard)\n",
    "columns_name.append('Avg_Utilization_Ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customer_Age',\n",
       " 'Gender',\n",
       " 'Education_Level_cat1',\n",
       " 'Education_Level_cat2',\n",
       " 'Education_Level_cat3',\n",
       " 'Education_Level_cat4',\n",
       " 'Education_Level_cat5',\n",
       " 'Education_Level_cat6',\n",
       " 'Marital_Status_cat1',\n",
       " 'Marital_Status_cat2',\n",
       " 'Marital_Status_cat3',\n",
       " 'Income_Category_cat1',\n",
       " 'Income_Category_cat2',\n",
       " 'Income_Category_cat3',\n",
       " 'Income_Category_cat4',\n",
       " 'Income_Category_cat5',\n",
       " 'Card_Category_cat1',\n",
       " 'Card_Category_cat2',\n",
       " 'Card_Category_cat3',\n",
       " 'Card_Category_cat4',\n",
       " 'Credit_Limit',\n",
       " 'Total_Revolving_Bal',\n",
       " 'Avg_Open_To_Buy',\n",
       " 'Total_Trans_Ct',\n",
       " 'Contacts_Count_12_mon',\n",
       " 'Total_Ct_Chng_Q4_Q1',\n",
       " 'Total_Trans_Amt',\n",
       " 'Dependent_count',\n",
       " 'Months_Inactive_12_mon',\n",
       " 'Months_on_book',\n",
       " 'Total_Relationship_Count',\n",
       " 'Total_Amt_Chng_Q4_Q1',\n",
       " 'Avg_Utilization_Ratio']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificazione\n",
    "\n",
    "Ora siamo pronti per addestrare un semplice modello di ML e lo potremmo fare piuttosto facilmente grazie al fatto che abbiamo gia' preparato i dati in modo da essere facilmente utilizzabili dagli oggetti di tipo **Predictor**.\n",
    "\n",
    "Utilizziamo come modello base il **Perceptron**. Il perceptron rappresenta la prima rete neurale - non possiamo sicuramente definire come ''deep''. L'algoritmo di apprendimento restituisce (se converge ðŸ˜€) un vettore di pesi. Nella formulazione base, l'unico iperparametro e' il **learning rate** $\\eta$.\n",
    "\n",
    "In SKL il modello e' implementato dalla classe **Perceptron**, un tipo di Predictor. La documentazione completa della classe si trova [qui](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html).\n",
    "\n",
    "Iniziamo a definire un Perceptron utilizzando tutti i valori di default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(feature_matrix, credit_card_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo **fit** implementa l'algoritmo di training in cui il vettore dei pesi viene modificato in base al learning rate e agli errori di classificazione durante il processo di addestramento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo come si comporta con alcuni elementi del training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True label : 0, Predicted Label: 0\n",
      "1 True label : 0, Predicted Label: 0\n",
      "2 True label : 0, Predicted Label: 0\n",
      "3 True label : 0, Predicted Label: 0\n",
      "4 True label : 0, Predicted Label: 0\n",
      "5 True label : 0, Predicted Label: 0\n",
      "6 True label : 0, Predicted Label: 0\n",
      "7 True label : 0, Predicted Label: 0\n",
      "8 True label : 0, Predicted Label: 0\n",
      "9 True label : 0, Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "sample_training = feature_matrix[:10,:]\n",
    "sample_labels = credit_card_label[:10]\n",
    "prediction_sample = perceptron.predict(sample_training)\n",
    "for i in range(len(sample_labels)):\n",
    "    print(i, 'True label : {}, Predicted Label: {}'.format(sample_labels[i],prediction_sample[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un risultato che potrebbe sembrare incoraggiante...\n",
    "\n",
    "Proviamo a contare quante volte sbaglio su un set di dati gia' visto..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted = perceptron.predict(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8899753086419753"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(all_predicted == credit_card_label)/len(credit_card_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non sembra male, ma ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8393086419753086"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predicted_dummy = np.zeros(len(credit_card_label))\n",
    "np.sum(all_predicted_dummy == credit_card_label)/len(credit_card_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il perceptron performance un po' meglio di un modello \"in tilt\", cioe' un modello che predice sempre la classe piu' frequente.\n",
    "\n",
    "Questa piccola differenza di performance ci permette di introdurre alcune trappole in cui siamo caduti. \n",
    "1) La valutazione delle performance deve essere effettuata su dati non ancora 'visti' dall'algoritmo di apprendimento (errore metodologico);\n",
    "2) La valutazione deve tenere conto del bilanciamento o meno delle classi (aspetto piu' complesso da gestire)\n",
    "\n",
    "In questa fase, esploriamo il **primo** problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Le metodologie di cross-validation possono essere utili per ottenere delle valutazioni piu' attendibili circa le proprieta' di **generalizzazione** del modello appreso, cioe' quanto il modello sara' in grado di **predirre correttamente la classe di appartenenza su dati non ancora osservati** - obiettivo finale del nostro task.\n",
    "\n",
    "Per prevenire il primo errore, e  di conseguenza l'overfitting dei dati, una pratica comune e' la **holdout cross-validation**. Si divide il dataset in due insiemi distinti: training e test dataset. Il primo e' usato per l'addestramento del modello, il secondo per la valutazione delle performance e la valutazione delle capacita' di generalizzazione.\n",
    "\n",
    "![](training_test.png)\n",
    "\n",
    "Un raffinamento del precedente schema e' l'utilizzo di un terzo insieme: **validation set**, grazie al quale viene fatto una model selection, i.e. vengono identificati l'insieme degli iperparametri ottimali. <br>\n",
    "In questo modo ho il vantaggio di non utilizzare il test set nella fase di learning riducendo il bias sulla stima della capacita' di generalizzare su nuovi dati.\n",
    "\n",
    "![](training_validation_test.png)\n",
    "\n",
    "Il modo in cui gli insiemi vengono partizionati influenza la stima delle performance. Per aumentare la robustezza della stima possiamo utilizzare **k-fold cross validation**.\n",
    "\n",
    "Prima, pero', vediamo come fare holdout cross-validation in SKL. SKL mette a disposizione il metodo **train_test_split** che mi restituisce una partizione in training e test sia della matrice delle feature sia del vettore delle label. E' possibile specificare la frazione di elementi che costituiscono i due insiemi.\n",
    "\n",
    "Nel nostro caso, il training set e' costituito dal 70% del campione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, credit_card_label, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7087, 33), (3038, 33))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8834759710335747"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train) # apprendo su training\n",
    "predicted_test = perceptron.predict(X_test) # predico sul test\n",
    "np.sum(predicted_test == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8739302172481896\n",
      "0.8640552995391705\n",
      "0.8531928900592495\n",
      "0.7919684002633312\n",
      "0.8633969716919025\n",
      "0.8759052007899935\n",
      "0.8601053324555629\n",
      "0.8736010533245556\n",
      "0.8828176431863067\n",
      "0.8574720210664911\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_matrix, credit_card_label, test_size = 0.3)\n",
    "    perceptron = Perceptron()\n",
    "    perceptron.fit(X_train, y_train) # apprendo su training\n",
    "    predicted_test = perceptron.predict(X_test) # predico sul test\n",
    "    print(np.sum(predicted_test == y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold Cross-validation\n",
    "\n",
    "Nella k-fold cross-validation - *k-fold CV* - il training set viene suddiviso in $k$ sottoinsiemi disgiunti - *fold* , dove per ogni fold $f$:\n",
    "1) i rimanenti $k-1$ fold sono utilizzati come training set\n",
    "2) il modello addestrato sui $k-1$ fold viene valutato sulla parte rimanente del dataset, cioe' $f$.\n",
    "\n",
    "In questo modo si ottengono $k$ modelli e $k$ misure di performance diverse. Si calcola, quindi una media delle performance. Una volta selezionato l'insieme degli iperparametri migliori, si riaddestra il modello sul training set completo e si effettua la valutazione del test set.\n",
    "\n",
    "![](kfold-cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attraverso il metodo [**cross_val_score**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html?highlight=cross_val_score#sklearn.model_selection.cross_val_score) possiamo usare direttamente k-CV e ottenere le misure di performance per ogni fold.\n",
    "\n",
    "Solitamente come valore da assegnare a $k$ si usa **5** o **10**, a seconda della dimensione del dataset. In SKL, se $K$ e' un intero vengono utilizzate le classi **KFold** o **StratifiedKFold**, come strategie di partizionamento di default (nota: la strategia basata sulla stratificazione verra' analizzata in seguito, in questo notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83145275, 0.88293371, 0.8659139 , 0.87720536, 0.85179958])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron_score = cross_val_score(perceptron, X_train, y_train, cv = 5)\n",
    "perceptron_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8618610604855605"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(perceptron_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approfondiremo le misure di performance in seguito.\n",
    "\n",
    "Come vedremo il parametro **cv** ammette come valori accettabbili, tipi di dato diverso, in modo da specificare quale strategia di divisione del set utilizzare in forma di classi SKL o iterable su un insieme di indici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una funzione simile a *cross_val_score* e' **cross_val_predict**. Restituisce per ogni elemento nel training set, la label predetta quando quell'elemento era nel fold di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7087,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_predict(perceptron, X_train, y_train, cv = 5).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Iterators\n",
    "La generazione delle fold dipende fortemente dal metodo di partizionamento/campionamento adottato e dalle assunzioni sul tipo di dato presente nel dataset.\n",
    "\n",
    "Se assumiamo che i dati siano i.i.d.  - indipendenti e identicamente distribuiti - il processo generativo dei dati e' unico e non ha memoria dei dati generati in passato. \n",
    "\n",
    "In questo caso possiamo utilizzare:\n",
    "- k-fold\n",
    "- Repeated k-fold\n",
    "- LOO\n",
    "- LPO\n",
    "- Shuffle and Split\n",
    "\n",
    "La classe **KFold** implementa il k-fold splitting della k-fold CV. Vedi https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8674189 , 0.8674189 , 0.83779972, 0.88575458, 0.87165021,\n",
       "       0.84626234, 0.86177715, 0.84180791, 0.81073446, 0.86864407])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Perceptron()\n",
    "kf = KFold(n_splits=10)\n",
    "cross_val_score(p, X_train, y_train, cv = kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante il metodo **split** posso ottenere l'insieme dei fold di training e il fold di test, espressi come indici della feature matrix. In sostanza indica quali righe inserire nel fold di training e quali inseire nel fold di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.11192662   4.          -6.           0.          -1.\n",
      "    0.          -1.          -4.          -4.          -4.\n",
      "   -4.          -3.          -5.           0.           0.\n",
      "   -4.          -4.          -4.           1.          -5.\n",
      "    0.23462171  -4.52213633   1.26803      0.4180946   -2.30161514\n",
      "    0.39368749  -3.80520232   1.65851149 -15.02805396   6.48086251\n",
      "    4.04939982  -0.80088207   0.66      ]]\n",
      "0.8674188998589563\n",
      "[[  1.32477671   2.           1.          -2.          -5.\n",
      "   -2.           1.          -1.          -3.          -3.\n",
      "   -2.           0.          -4.          -4.           1.\n",
      "   -1.          -4.          -1.          -1.          -2.\n",
      "   -3.4387218   -9.10400562  -1.72546584   1.48399923  -0.42861642\n",
      "   -2.57556399  -8.40807656   1.36232157 -15.94158444   8.03833993\n",
      "    6.65752759  -2.24968872   0.956     ]]\n",
      "0.8674188998589563\n",
      "[[ -0.71224369   4.           0.          -2.          -3.\n",
      "   -2.           0.           0.          -2.          -4.\n",
      "   -1.           4.          -7.          -1.          -1.\n",
      "   -2.          -5.           0.           3.          -5.\n",
      "    2.31287594  -6.57062544   3.56617837   7.40017616  -0.81594988\n",
      "   -0.81373952  -6.09421076   1.86573038 -14.74250993   4.91329644\n",
      "    1.37266797  -4.05956907  -2.162     ]]\n",
      "0.8377997179125529\n",
      "[[  4.11036644   5.          -6.          -1.          -2.\n",
      "    1.          -1.           0.          -2.          -3.\n",
      "   -4.           3.          -7.          -4.           1.\n",
      "   -2.          -4.          -2.          -2.          -1.\n",
      "   -3.61963111 -11.85523542  -1.42380171   3.70339141  -0.12798205\n",
      "   -4.33738846  -4.70644093   6.24850479 -17.95026149   7.30278611\n",
      "    1.05808601  -2.37004662  -4.733     ]]\n",
      "0.8857545839210155\n",
      "[[ -1.50615845   7.          -4.           0.          -1.\n",
      "   -3.          -1.          -3.          -3.          -8.\n",
      "   -1.          -2.          -6.          -2.          -1.\n",
      "   -1.          -4.          -2.          -2.          -4.\n",
      "   -2.15232613  -4.38369642  -1.13613032  -2.29376178  -0.90074035\n",
      "    2.39699736  -5.38889229   2.42845321 -16.86031236   8.99441793\n",
      "    2.07043596  -4.01794582   3.041     ]]\n",
      "0.8716502115655853\n",
      "[[  4.48690605   5.          -3.           0.          -2.\n",
      "   -2.           0.           1.          -1.          -3.\n",
      "   -2.          -1.          -2.          -3.           0.\n",
      "    0.          -5.           2.          -1.          -2.\n",
      "    0.92268562 -10.75333802   2.85361538   4.27683186  -5.03173267\n",
      "   -0.42919059  -6.09916954   3.1390809  -15.58874712  10.73338855\n",
      "    5.98262762  -3.29579842  -1.466     ]]\n",
      "0.846262341325811\n",
      "[[  2.36370468   4.          -3.           3.          -2.\n",
      "   -4.           0.          -3.          -1.          -7.\n",
      "   -1.           0.          -5.          -1.          -2.\n",
      "   -1.          -7.           4.          -1.          -5.\n",
      "   -3.2565907   -9.36401968  -1.47710067   1.89548717  -2.8247801\n",
      "   -2.96011293   2.9683643    2.3987962  -14.498798     4.31155209\n",
      "    2.04756793  -0.43980836  -5.685     ]]\n",
      "0.8617771509167842\n",
      "[[  2.52930177   3.          -3.           2.           0.\n",
      "   -3.          -4.          -2.          -2.          -7.\n",
      "   -1.           3.          -4.          -4.           0.\n",
      "   -5.          -2.          -4.          -3.          -1.\n",
      "    2.08531485  -8.05621926   3.65688503   4.11487935  -1.73472769\n",
      "    1.91402656  -1.59291299   3.43527083 -12.41685165   5.80889542\n",
      "    5.3534637   -6.35088103  -1.737     ]]\n",
      "0.8418079096045198\n",
      "[[ -0.4196301    6.          -3.           0.           1.\n",
      "   -1.          -3.           1.          -2.          -4.\n",
      "    1.          -1.          -3.          -1.           0.\n",
      "    0.          -4.           3.           1.          -5.\n",
      "   -3.50496945  -8.566409    -1.95116606   6.57720029   1.02860838\n",
      "    0.58139268  -2.41165223   1.33266455 -18.43768536   9.59268442\n",
      "    1.68724992  -3.17544051   0.65      ]]\n",
      "0.8107344632768362\n",
      "[[ -2.04378525  10.          -6.           2.          -4.\n",
      "   -2.          -1.           3.          -1.          -1.\n",
      "   -6.          -1.           0.           2.          -3.\n",
      "   -6.          -8.           3.          -2.          -1.\n",
      "   -3.72819549 -13.20520028  -1.33029415   5.09980773  -0.73434479\n",
      "   -3.20159833  -6.69836352   2.90220501 -16.45291236   6.99787229\n",
      "    3.6890818   -4.82333972   1.75      ]]\n",
      "0.8686440677966102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cls = []\n",
    "for train_idx, test_idx in kf.split(X_train, y_train):\n",
    "    x_train_fold, y_train_fold = X_train[train_idx,:], y_train[train_idx]\n",
    "    p = Perceptron()\n",
    "    p.fit(x_train_fold, y_train_fold)\n",
    "    print(p.coef_)\n",
    "    x_val_fold, y_val_fold = X_train[test_idx,:], y_train[test_idx]\n",
    "    fold_predicted = p.predict(x_val_fold)\n",
    "    print(np.sum(fold_predicted == y_val_fold)/len(y_val_fold))\n",
    "    all_cls.append(p)\n",
    "all_cls[3]\n",
    "\n",
    "p_all = Perceptron()\n",
    "p_all.fit(X_train, y_train)\n",
    "p_all.predict(X_test)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo notare come nell'esempio precedente sul perceptron, i valori di performance possono oscillare ed influenzare la performance media. \n",
    "\n",
    "Una soluzione per ridurre l'effetto delle oscillazioni nel calcolo della performance media e' ripetere la k-fold CV per diverse volte e riportare la performance media calcolata su tutti i fold di tutte le ripetizioni. Ovviamente, il partizionamento tra due ripetizioni non e' lo stesso.<br>\n",
    "Questo approccio e' utile con dataset di piccola/media dimensione e che non richiedono un elevato costo di computazione nella fase di training, anche se e' facile parallelizzare il processo (SKL lo fa di default)\n",
    "\n",
    "SKL fornisce un'implementazione di tale metodo mediante la classe **RepeatedKFold** (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold). Si devono specificare i valori di $K$ e il numero di ripetizioni *n_repeats*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88857546, 0.85754584, 0.86318759, 0.78984485, 0.89562764,\n",
       "       0.82369535, 0.87306065, 0.83474576, 0.90536723, 0.85028249,\n",
       "       0.8674189 , 0.89562764, 0.84062059, 0.80394922, 0.88152327,\n",
       "       0.87165021, 0.85895628, 0.86440678, 0.87853107, 0.84887006,\n",
       "       0.87729196, 0.89421721, 0.88011283, 0.87306065, 0.88011283,\n",
       "       0.88293371, 0.85190409, 0.89548023, 0.85451977, 0.88700565])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rkf = RepeatedKFold(n_splits=10, n_repeats=3)\n",
    "cross_val_score(p, X_train, y_train, cv = rkf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da notare il parametro *n_jobs* tramite cui specifico quanti core utilizzare in parallelo per eseguire CV.\n",
    "\n",
    "E' possibile anche valutare l'effetto del numero di ripetizioni sulle performance medie del classificatore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x16fa7263df0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7264280>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7b1f130>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81a81c0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa6df1ca0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa818ba00>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7234d90>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7234c10>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7ff9220>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80068b0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa822d370>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa822d700>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa821e070>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa821e400>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7241fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8025dc0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa6de93a0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8124a30>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81d3be0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81d3f70>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81d08e0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81d0c70>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81c95e0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81c9970>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80712e0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8071670>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80aafa0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8093370>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa807cca0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7c0a070>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x16fa7264520>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7264e50>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81a8700>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81a8e80>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa818b580>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa818be20>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa802aa30>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa802a850>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8006e80>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8006610>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa822da90>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa822de20>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa821e790>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa821eb20>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa6d212b0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8110190>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8124c70>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa820c8b0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81b2340>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81b26d0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81ca040>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81ca3d0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81c9d00>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80900d0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8071a00>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8071d90>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8093700>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8093a90>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7c0a400>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7c0a790>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x16fa7263070>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7b1ff10>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa5daa460>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7234070>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7ff90d0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80d0fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa822aca0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa820c9a0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80c0dc0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81d3850>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81d0550>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81c9250>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8090f10>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80aac10>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa807c910>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x16fa72646d0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81a8df0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa818beb0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa802aa00>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8006550>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa822a1f0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa821eeb0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8110880>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa820cd60>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81b2a60>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81ca760>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8090460>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80aa160>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8093e20>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7c0ab20>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x16fa7b1f190>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa5daa7f0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7234250>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7ff9b80>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80d0c10>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa822a910>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa820c610>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80c02b0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81d34c0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81d01c0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81cae80>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa8090b80>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80aa880>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa807c580>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7c0e280>],\n",
       " 'means': [<matplotlib.lines.Line2D at 0x16fa7b1f5b0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa5daad90>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa64d7e50>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7ff9460>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80d01c0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa822a580>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa820c280>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80b1880>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81d3130>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81b2df0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa81caaf0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80907f0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa80aa4f0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa807c1f0>,\n",
       "  <matplotlib.lines.Line2D at 0x16fa7c0aeb0>]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def valuta_modello_ripetizione(X, y, repeats):\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=repeats, random_state=1)\n",
    "    model = Perceptron()\n",
    "    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "repeats = range(1,16)\n",
    "results = list()\n",
    "for i in repeats:\n",
    "    scores = valuta_modello_ripetizione(feature_matrix, credit_card_label, i)\n",
    "    results.append(scores)\n",
    "plt.boxplot(results, labels = [str(i) for i in repeats], showmeans=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sempre nel setting iid, possiamo generare un insieme di split training/test in cui viene violata la condizione di insieme disgiunto tra le fold. In questo caso appliciamo una random permutation CV.\n",
    "\n",
    "In SKL tale strategia e' fornita dalla classe **ShuffleSplit**. Per ogni iterazione - splitting, la feature matrix viene permutata e una sua frazione di righe viene assegnata al test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.848081264108352"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spf = ShuffleSplit(n_splits = 10, test_size = 0.25)\n",
    "perceptron_score = cross_val_score(p, X_train, y_train, cv = spf, n_jobs=-1)\n",
    "perceptron_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo modo si ha un controllo maggiore sulla porzione di elementi nel test e si perde la dipendenza tra $k$ e *test_size*\n",
    "\n",
    "#### Stratification\n",
    "In alcuna situazione assistiamo - come nel nostro caso - ad un forrte sbilanciamento nella distribuzione delle classi nel vettore delle label. In questi casi si raccomanda di utilizzare un campionamento stratitificato implementato dalle classi SKL **StratifiedKFold** e **StratifiedShuffleSplit**. Tali classi assicurano che la distribuzione delle etichette in ogni fold approssimi quella dell'intero vettore delle etichette\n",
    "\n",
    "Nel caso di StratifiedKFold avremo la suddivisione in figura:\n",
    "\n",
    "![](stratified_kfold_cv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----K-fold CV----------\n",
      "Train: 0.17 vs Test: 0.10\n",
      "Train: 0.17 vs Test: 0.07\n",
      "Train: 0.17 vs Test: 0.05\n",
      "Train: 0.16 vs Test: 0.14\n",
      "Train: 0.16 vs Test: 0.21\n",
      "Train: 0.15 vs Test: 0.21\n",
      "Train: 0.16 vs Test: 0.21\n",
      "Train: 0.16 vs Test: 0.20\n",
      "Train: 0.16 vs Test: 0.15\n",
      "Train: 0.15 vs Test: 0.25\n",
      "----Stratified K-fold CV-------\n",
      "Train: 0.16 vs Test: 0.16\n",
      "Train: 0.16 vs Test: 0.16\n",
      "Train: 0.16 vs Test: 0.16\n",
      "Train: 0.16 vs Test: 0.16\n",
      "Train: 0.16 vs Test: 0.16\n",
      "Train: 0.16 vs Test: 0.16\n",
      "Train: 0.16 vs Test: 0.16\n",
      "Train: 0.16 vs Test: 0.16\n",
      "Train: 0.16 vs Test: 0.16\n",
      "Train: 0.16 vs Test: 0.16\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "print('----K-fold CV----------')\n",
    "for train, test in kf.split(feature_matrix, credit_card_label):\n",
    "    y_train, y_test = credit_card_label[train], credit_card_label[test]\n",
    "    print('Train: {:.2f} vs Test: {:.2f}'.format(sum(y_train == 1)/len(y_train), sum(y_test == 1)/len(y_test)))\n",
    "print('----Stratified K-fold CV-------')\n",
    "for train, test in skf.split(feature_matrix, credit_card_label):\n",
    "    y_train, y_test = credit_card_label[train], credit_card_label[test]\n",
    "    print('Train: {:.2f} vs Test: {:.2f}'.format(sum(y_train == 1)/len(y_train), sum(y_test == 1)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo stesso principio si applica alla metodologia di shuffle and split:\n",
    "\n",
    "![](stratified_shufflesplit_cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measure for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viste le diverse strategie di cross-validation, affrontiamo un secondo argomento essenziale: le misure di performance.\n",
    "\n",
    "Abbiamo gia' visto che il perceptron utilizzato in precedenza non era meglio di un classificatore che prediceva sempre l'etichetta piu' frequente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_dum_cls = DummyClassifier(strategy='most_frequent')\n",
    "uni_dum_cls = DummyClassifier(strategy='uniform')\n",
    "st_dum_cls = DummyClassifier(strategy='stratified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, credit_card_label, test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8388599364107957"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mf_dum_cls, X_train, y_train, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cio' mostra come l'accuracy non e' una buona misura di performance, specialmente quando il vettore delle etichette e' sbilanciato.\n",
    "\n",
    "Un modo migliore per valutare le performance di un classificatore e' la **confusion matrix** $C$. L'elemento $C_{ij}$ della matrice corrisponde al numero di osservazioni che hanno etichetta $i$ e che sono state classificate con etichetta $j$. Nel caso di classificazione binario:\n",
    "1) $C_{00}$ = veri negativi\n",
    "2) $C_{01}$ = falsi positivi\n",
    "3) $C_{10}$ = falsi negativi\n",
    "4) $C_{11}$ = veri positivi\n",
    "\n",
    "![](confusion_matrix.png)\n",
    "\n",
    "Per calcolare la confusion matrix, necessitiamo di:\n",
    "1) il vettore delle etichette\n",
    "2) il vettore delle etichette predette\n",
    "\n",
    "Per ottenere il secondo elemento possiamo utilizzare il metodo  *cross_val_predict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predicted = cross_val_predict(p, X_train, y_train, cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine per calcolare la confusion matrix utilizziamo il metodo **confusion matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5311,  634],\n",
       "       [ 478,  664]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_train_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni riga corrisponde ad una classe _attuale_, mentre ogni colonna una _classe predetta_.\n",
    "\n",
    "Dalla matrice di confusione posso calcolare l'accuratezza delle predizioni positive, i.e. la **precision**:\n",
    "$$precision = \\frac{C_{11}}{C_{11} + C_{01}} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "a cui si accompagna un'altra metrica: la **recall** o **sensitivity** o **true positive rate**:\n",
    "$$recall = \\frac{C_{11}}{C_{11} + C_{10}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "In SKL precision e recall sono restituite dai metodi **precision_score** e **recall_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5115562403697997"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_train, y_train_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5814360770577933"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_train, y_train_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo le differenze confrontandoci con i due dummy classifier definiti in precedenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mf = cross_val_predict(mf_dum_cls, X_train, y_train, cv = 10)\n",
    "y_train_un = cross_val_predict(uni_dum_cls, X_train, y_train, cv = 10)\n",
    "y_train_st = cross_val_predict(st_dum_cls, X_train, y_train, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.1578204404291361, 0.17106382978723406)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_train, y_train_mf), precision_score(y_train, y_train_un), precision_score(y_train, y_train_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel caso di dummy che predice la singola classe piu' frequente, la colonna nella confusion matrix relativa alla classe 1 e' composta da 0 => 0/0. Nei rimanenti casi, si puo' intuire il risultato se pensiamo al processo di classificazione implementato come una selezione. Nel caso predica 1, seleziono l'elemento del campione e mi chiedo quale sia la sua etichetta. In entrambi i casi vengono rispettate le proporzioni della classe nel campione originario. In un caso seleziono meno elementi (stratified), perche' le etichette sono sbilanciate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.489492119089317, 0.17600700525394045)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_train, y_train_mf), recall_score(y_train, y_train_un), recall_score(y_train, y_train_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel primo caso la recall e' 0 perche' $C_{11}=0$. Per spiegare i rimanenti risultati, si selezionano i campioni con etichetta vera = 1, nel caso uniforme ottengo il 50% di elementi con etichetta 1, nel caso stratified ottengo solo il 16% con etichetta 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2962, 2983],\n",
       "       [ 583,  559]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_train_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4971,  974],\n",
       "       [ 941,  201]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_train_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sia in recall che precision, il perceptron si comporta leggermente meglio di un random guesser.\n",
    "\n",
    "In generale *una soluzione basata su perceptron non e' affidabile nel caso mi segnali un cliente 'in uscita', e neanche in grado di identificare un cliente 'in uscita'*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere un modo semplice per confrontare due classificatori, viene spesso impiegato l'indicatore $F_{1}$, definito come la media armonica tra precision e recall. Penalizza un classificatore con precision e recall sbilanciate.\n",
    "$F_1 = \\frac{2}{\\frac{1}{precision}+\\frac{1}{recall}}$. Questo indicatore viene implementato dal metodo **f1_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5442622950819672"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_train_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/Recall Tradeoff\n",
    "Utilizziamo ora un logit classifier per il problema dell'identificazione dei churner dal momento che restituisce anche un prediction score. Nel caso del classificatore in questione, corrisponde al valore prima della funzione di threshold, i.e. $\\phi(\\mathbf{w}^T\\mathbf{x})$. Esso indica la confidenza sulla classe predetta. In SKL un logit classifier e' implementato dalla classe **LogisticRegression**, il quale rende disponibile il metodo **decision_function** per ottenere gli score di confidenza associati ad ogni istanza predetta.\n",
    "\n",
    "In questo modo possiamo agire sulla funzione di soglia e modificare la predizione finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato gli score, possiamo utilizzare il metodo **precision_recall_curve** per calcolare precision e recall al variare del valori di soglia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oppure posso visualizzare la precision in funzione della recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo tipo di analisi ci permette di scegliere il valore in base alle nostre esigenze. Per esempio se volessimo aumentare la fino a 0.9 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "La curva **Receiver operating characteristic - ROC** e' molto simile alla curva precision/recall ma visualizza la recall in funzione del **false positive rate**:\n",
    "$$FPR = \\frac{C_{01}}{C_{01}+C_{00}} = \\frac{FP}{FP+TN} = 1 - specificity$$\n",
    "Per visualizzare la curva ROC applichiamo lo stesso procedimento precedente utilizzo il metodo **roc_curve**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All'aumentare della recall, aumenta la produzione di falsi positivi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Learning and validation curve\n",
    "Learning e validation curve  sono altri semplici strumenti per la valutazione delle performance e l'identificazoine di problemi di overfitting o underfitting\n",
    "\n",
    "se un modello e' troppo complesso, il modello tende a overfittare sui dati e non generalizza bene. In questo caso puo' aiutare aumentare la dimensione del training set - molto spesso non e' una soluzione attuabile. In questo caso si possono visualizzare le accuracy del training e del validation set in funzione della dimensione del training set = *learning curve* \n",
    "\n",
    "![](bias-variance.png)\n",
    "\n",
    "Nel primo caso abbiamo un modello con un bias elevato in quanto TS e V accuracy sono rispetto al target -> passare ad un modello con meno assunzioni, oppure aumentare grado di liberta' del modello (cfr. regolarizzazione). In questo caso abbiamo underfitting.\n",
    "\n",
    "Nel secondo caso abbiamo un alta varianza, in quanto raggiungiamo l'accuracy \"possibile\" ma esiste un gap tra TS e V accuracy. In questo caso abbiamo overfitting -> collezionare piu' training data, ridurre la complessita' del modello o introdurre regolarizzazione, oppure feature selection.\n",
    "\n",
    "In SKL posso valutare la learning curve mediante il metodo **learning_curve**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('Dimensione del training set')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0.6, 1.03])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ulteriore strumento di valutazione di problemi di over/under-fitting e' la **validation curve**. In questo caso, si varia il valore dei parametri del modello.\n",
    "\n",
    "Nel caso di LogisticRegression, il parametro $C$ e' l'inverso del parametro di regolarizzazione $\\lambda$. All'aumentare del valore di C, diminuisce l'effetto di regolarizzazione, diminuisco la varianza\n",
    "\n",
    "In SKL posso valutare la validation curve mediante il metodo **validation_curve**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(range_c, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "\n",
    "ax.fill_between(range_c,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(range_c, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "\n",
    "ax.fill_between(range_c,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('Parametro C')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0.6, 1.03])\n",
    "ax.set_xlim([0., 1.03])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso per valori di C superiori a 0.01, inizia un fenomeno di overfitting indicato da una differenza crescente tra accuracy nel training e accuracy nella validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "La classificazione multiclasse e' applicabile in un contesto in cui il label vector assume piu' di due valori discreti. Alcuni algoritmi di ML sono in grado di gestire un vettore multiclasse (Random Forest e Naive Bayes), mentre alcuni sono strettamente binari (SVM, Linear Classifier). In questo caso esistono una serie di strategie da applicare ad un problema di classificazione multiclasse utilizzando classificatori binari:\n",
    "1. OvA (one-versus-all): si addesstrano N classificatore, uno per ognuna delle N classi. Per ognuno degli N classificatori, si considera lo score di classificazione ottenuto, e si restituisce la classe associata allo score con valore massimo.\n",
    "2. OvO (one-versus-one): si addestrano $N(N-1)/2$ classficiatori, uno per ogni coppia di classi. Per classificare un nuovo campione, si applicano tutti i classificatori e si restituisce la classe che vince piu' round di classificazione.\n",
    "\n",
    "Dal punto di vista computazionale, il primo approccio e' piu' efficiente  - N contro $N^2$ - rispetto a OvO, tranne nel caso in cui l'algoritmo di apprendimento non e' efficiente in funzione del numero di campioni $M$. Nel caso di OvA, la complessita' e' $NM$, mentre nel caso di OvO la complessita' e' $(N-1)M$ nel caso di label vector bilanciato, tuttavia, in caso di architettura multi-core si possono apprendere i classificatori nel confronto OvO in modo parallelo.\n",
    "\n",
    "\n",
    "In SKL tutti i classificatori implementano nativamente la classificazione multiclass. Nel caso si volesse sperimentare con una soluzione diversa da quella di default il modulo **multiclass** rende disponibili due classi **OneVsRestClassifier** e **OneVsOneClassifier**, che rispettivamente implementano OvA e OvO. Il costruttore della classe accetta un qualsiasi oggetti di tipo classifier.\n",
    "\n",
    "Per questo tipo di classificazione utilizziamo il data MNIST, il quale contiene come campione delle immagini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_instance(X):\n",
    "    size = X.shape[0]\n",
    "    return X[np.random.choice(np.arange(size))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso ottenere tutti gli score delle varie classi utilizzando il metodo **decision_function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E controllare la classe con lo score maggiore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo ora la classe **OneVsOneClassifier** per creare una strategia di decisione dell'etichetta diversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso accedere a tutti i classificatori binari sulle coppie mediante l'attributo **estimators_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso posso utilizzare la cross-validation utilizzando accuracy come misura di performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E utilizzando una standardizzazione sulla feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix):\n",
    "    \"\"\"If you prefer color and a colorbar\"\"\"\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
