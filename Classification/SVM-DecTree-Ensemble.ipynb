{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Data preprocessing and trasformation (ETL)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, FunctionTransformer, Binarizer, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml, load_iris, make_moons, make_classification\n",
    "\n",
    "\n",
    "# Math and Stat modules\n",
    "import numpy as np\n",
    "from scipy.stats import sem, randint\n",
    "from random import choice\n",
    "\n",
    "# Supervised Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, StratifiedKFold, RepeatedKFold, ShuffleSplit, StratifiedShuffleSplit, learning_curve, validation_curve, cross_validate\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "#from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "# Unsupervised Learning\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_decision(ax, clf, X, y, **params):\n",
    "    x0, x1 = X[:, 0], X[:, 1]\n",
    "    print(x1)\n",
    "    xx, yy = make_meshgrid(x0, x1)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "    \n",
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True, alpha = 0.8):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=alpha)\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "        plt.axis(axes)\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, columns_name = [pickle.load(open('credit_card.pkl','rb'))[k] for k in ['features','labels','columns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "X_iris = iris_data['data'][:,2:]\n",
    "y_iris = (iris_data[\"target\"] == 0).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "Support Vector Machine - SVM - sono un modello molto performante e versatile nell'ambito del ML. Utilizzate per la classificazione lineare e non lineare, la regressione e persino per l'identificazione di outlier (outlier detection).\n",
    "\n",
    "In Scikit-learn SVMs sono implementate dalle classi **SVC** e **LinearSVC** contenute nel modulo **svm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_or_versicolor = (y_iris == 0) | (y_iris == 1) # seleziono solo due classi, anziche' le tre disponibili\n",
    "X_iris_2 = X_iris[setosa_or_versicolor] # feature matrix\n",
    "y_iris_2 = y_iris[setosa_or_versicolor] # label\n",
    "\n",
    "# SVM Classifier model\n",
    "svm_clf = LinearSVC(C=200, max_iter=50000)\n",
    "svm_clf.fit(X_iris_2, y_iris_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0, 5.5, 200)\n",
    "pred_1 = 5*xx - 20\n",
    "pred_2 = xx - 1.8\n",
    "pred_3 = 0.1 * xx + 0.5\n",
    "\n",
    "def plot_svc_decision_boundary(ax, svm_clf, xx):\n",
    "    w = svm_clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    yy = a * xx - (svm_clf.intercept_[0]) / w[1]\n",
    "    \n",
    "    margin = 1 / np.sqrt(np.sum(svm_clf.coef_ ** 2))\n",
    "    yy_down = yy - np.sqrt(1 + a ** 2) * margin\n",
    "    yy_up = yy + np.sqrt(1 + a ** 2) * margin\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    ax.plot(xx, yy, 'k-')\n",
    "    ax.plot(xx, yy_down, 'k--')\n",
    "    ax.plot(xx, yy_up, 'k--')\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(xx, pred_1, \"g--\", linewidth = 2, label='Separazione 1')\n",
    "ax.plot(xx, pred_2, \"m-\", linewidth = 2, label='Separazione 2')\n",
    "ax.plot(xx, pred_3, \"r-\", linewidth = 2, label='Separazione 3')\n",
    "plot_svc_decision_boundary(ax, svm_clf, xx)\n",
    "ax.plot(X_iris_2[:, 0][y_iris_2==1], X_iris_2[:, 1][y_iris_2==1], \"bs\")\n",
    "ax.plot(X_iris_2[:, 0][y_iris_2==0], X_iris_2[:, 1][y_iris_2==0], \"yo\")\n",
    "ax.set_xlabel(\"Petal length\", fontsize=14)\n",
    "ax.axis([0, 5.5, 0, 2])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le due classi sono linearmente separabili e esiste un ricco insieme di iperpiani separatori, tra cui quelli rossi e viola in figura. Tuttavia le ultime sono molto vicine alle classi del training set, ponendo limitazioni sulle performance sul validation e test set (riduzioe della capacita' di generalizzazione). In neretto sono stati riportati \n",
    "l'iperpiano identificato da SVM e i margini identificati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel caso del dataset Iris, la separazione e' ben evidente, cosi' come la la classificazione mediante hard margin e support vector. Nel caso di un sottoinsieme di feature del nostro dataset di esempio, si ottengono risultati molto meno significativi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_subset = X[:,[20,27]]\n",
    "# SVM Classifier model\n",
    "svm_clf = LinearSVC(C=1, max_iter=50000)\n",
    "svm_clf.fit(X_subset, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(12,5)).add_subplot()\n",
    "plot_decision(ax, svm_clf, X_subset, y ,cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X_subset[:, 0], X_subset[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k', alpha=0.5)\n",
    "w = svm_clf.coef_[0]\n",
    "b = svm_clf.intercept_[0]\n",
    "x0 = np.linspace(-3, 4, 200)\n",
    "iperpiano_sep = -w[0]/w[1] * x0 - b/w[1]\n",
    "margine = 1/w[1]\n",
    "margine_up = iperpiano_sep + margine\n",
    "margine_down = iperpiano_sep - margine\n",
    "ax.plot(x0, iperpiano_sep, \"k-\", lw=2)\n",
    "plt.plot(x0, margine_up, \"k--\", lw=2)\n",
    "plt.plot(x0, margine_down, \"k--\", lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo di hard margin classification funziona solamente se le classi sono linearmente separabili ed e' sensibile agli outlier.<br>\n",
    "Vediamo quale puo' essere, invece, l'effetto di uno scaling delle feature sull'iperpiano e i margini identificati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset esempio con outlier\n",
    "Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\n",
    "ys = np.array([0, 0, 1, 1])\n",
    "#Training\n",
    "svm_clf = SVC(kernel='linear',C=100)\n",
    "svm_clf.fit(Xs, ys)\n",
    "\n",
    "fig = plt.figure(figsize=(12,3.2))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")\n",
    "ax1.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")\n",
    "plot_svc_decision_boundary(ax1, svm_clf, np.linspace(0,6,100))\n",
    "ax1.set_xlabel(\"$x_0$\", fontsize=20)\n",
    "ax1.set_ylabel(\"$x_1$  \", fontsize=20, rotation=0)\n",
    "ax1.set_title(\"Unscaled\", fontsize=16)\n",
    "ax1.axis([0, 6, 0, 90])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(Xs)\n",
    "svm_clf.fit(X_scaled, ys)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")\n",
    "ax2.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")\n",
    "plot_svc_decision_boundary(ax2, svm_clf, np.linspace(-2,2,100))\n",
    "ax2.set_xlabel(\"$x_0$\", fontsize=20)\n",
    "ax2.set_title(\"Scaled\", fontsize=16)\n",
    "ax2.axis([-2, 2, -2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per gestire il caso di classi non linearmente separabili vengono introdotte nel modello le variabilie di slack che misurano il livello di violazione del margine. Si deve quindi ottimizzare il margine e allo stesso stempo ridurre il numero di violazioni - interessi contropposti.\n",
    "\n",
    "Attraverso il parametro $C$ posso controllare il trade-off ampiezza del margine e numero di violazioni. Se diminuisco il valore di C privilegio il margine - aumentandone l'ampiezza - e aumento il numero di violazioni. Aumento l'effetto di regolarizzazione, riducendo la probabilit' di overfitting. Se aumento C ho la situazione opposta e posso incappare piu' facilmente in overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio (aula virtuale) - 10 min + discussione dei risultati\n",
    "Utilizzando il dataset per churn prediction, si valutino le learning di una serie di SVM lineari variando l'iperparametro $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = # definire un insieme di valori di C tenendo in considerazione le precedenti osservazioni sul suo effetto \n",
    "fig = plt.figure(figsize=(18,3.2))\n",
    "for i, c in enumerate(Cs):\n",
    "    print('Training SVM per C =', c)\n",
    "    # definire SVM con il parametro specificato\n",
    "    # calcolare la learning curve per il classificatore definito\n",
    "\n",
    "    print('Training per {} finito'.format(c))\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    ax = fig.add_subplot(130+(i+1))\n",
    "    ax.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "    ax.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "    ax.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "    ax.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "    ax.grid()\n",
    "    ax.set_ylim((0.8,1))\n",
    "    ax.set_xlabel('Dimensione del training set')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVM\n",
    "\n",
    "Molti dataset non sono linearmente separabili. Un approccio per gestire la non linearita' e' la creazione di feature polinomiali. Il problema e' che introducendo feature polinomiali con alto grado del polinomio si introduce un elevato numero di feature aggiuntive che rendono il processo di training computazionalmente oneroso. Nel caso delle SVM non esiste la necessita' di creare delle feature nuove dal momento che si puo' utilizzare il kernel trick (vedi formulazione duale del problema di ottimizzazione quadratica). Attraverso il kernel trick non ho un'esplosione di feature e posso introdurre non linearita'.\n",
    "\n",
    "Vediamo come applicare SVM non lineari ad un classico caso di dataset non linearmente separabile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moon, y_moon = make_moons(n_samples=1000, noise=0.15, random_state=42)\n",
    "\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X_moon[:, 0][y_moon==0], X_moon[:, 1][y_moon==0], \"bs\")\n",
    "    plt.plot(X_moon[:, 0][y_moon==1], X_moon[:, 1][y_moon==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "plot_dataset(X_moon, y_moon, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly3_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=2, coef0=1, C=5))\n",
    "    ])\n",
    "poly3_kernel_svm_clf.fit(X_moon, y_moon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso aumentare il grado del polinomio ma potrei aumentare la probabilita' di overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly6_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=7, coef0=1, C=5))\n",
    "    ])\n",
    "poly6_kernel_svm_clf.fit(X_moon, y_moon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_predictions(poly3_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X_moon, y_moon, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(r\"$d=2, r=1, C=5$\", fontsize=18)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions(poly6_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(r\"$d=7, r=1, C=5$\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il coefficiente _coef0_ amplifica gli effetti delle feature con alto degree. Anche in questo caso ha influenza su overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly6_1_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=6, coef0=1, C=5))\n",
    "    ])\n",
    "poly6_1_kernel_svm_clf.fit(X_moon, y_moon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly6_10_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=6, coef0=10, C=5))\n",
    "    ])\n",
    "poly6_10_kernel_svm_clf.fit(X_moon, y_moon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_predictions(poly6_1_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X_moon, y_moon, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(r\"$d=6, r=1, C=5$\", fontsize=18)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions(poly6_10_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(r\"$d=6, r=10, C=5$\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una seconda opzione per gestire la non linearita' e' aggiungere feature calcolate utilizzando una funzione di similarita'che misura quanto un'istanza $\\mathbf{x}$ e' simile ad un landmark $\\mathbf{l}$. Per esempio, una funzione utilizzata e' Gaussian Radial Basis Function - RBF.\n",
    "$$\\phi(\\mathbf{x},\\mathbf{l}) = exp(-\\gamma||\\mathbf{x}-\\mathbf{l}||^2)$$\n",
    "Ottengo una forma a campana che assume valori tra 0 (punti distanti e dissimili) e 1 (punti uguali) con $\\gamma$ che agisce sulla dispersione della campana. Diminuendo $\\gamma$ aumento la dispersione e punti piu' lontani sono considerati piu' simili al landmark, rispetto ad un $\\gamma$ piu' piccolo.\n",
    "\n",
    "Nel nostro caso i landmark corrispondono a tutte le istanze del dataset - *m* - e se applicassi RBF senza kernel trick otterrei *m* features e di conseguenza una matrice *m* x *m*. \n",
    "\n",
    "Anche in questo caso vale il kernel trick e in SKL posso utilizzare la solita classe **SVC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma1, gamma2 = 0.1, 5\n",
    "C1, C2 = 0.001, 1000\n",
    "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
    "\n",
    "svm_clfs = []\n",
    "for gamma, C in hyperparams:\n",
    "    rbf_kernel_svm_clf = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
    "        ])\n",
    "    rbf_kernel_svm_clf.fit(X_moon, y_moon)\n",
    "    svm_clfs.append(rbf_kernel_svm_clf)\n",
    "\n",
    "plt.figure(figsize=(11, 7))\n",
    "\n",
    "for i, svm_clf in enumerate(svm_clfs):\n",
    "    plt.subplot(221 + i)\n",
    "    plot_predictions(svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "    plot_dataset(X_moon, y_moon, [-1.5, 2.5, -1, 1.5])\n",
    "    gamma, C = hyperparams[i]\n",
    "    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma1, gamma2 = 0.1, 2\n",
    "C1, C2 = 0.01, 5\n",
    "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
    "\n",
    "train_sizes, train_means, test_means, test_stds, train_stds = [],[],[],[],[]\n",
    "for gamma, C in hyperparams:\n",
    "    rbf_kernel_svm_clf = SVC(kernel=\"rbf\", gamma = gamma, C = C)\n",
    "    train_size, train_scores, test_scores = learning_curve(rbf_kernel_svm_clf,\n",
    "                                                       X=X,\n",
    "                                                       y=y,\n",
    "                                                       train_sizes=np.linspace(0.1,1.0,10),\n",
    "                                                       cv=5,\n",
    "                                                       n_jobs=-1)\n",
    "    print('fatto {},{}'.format(gamma,C))\n",
    "    train_means.append(np.mean(train_scores, axis=1))\n",
    "    train_stds.append(np.std(train_scores, axis=1))\n",
    "    test_means.append(np.mean(test_scores, axis=1))\n",
    "    test_stds.append(np.std(test_scores, axis=1))\n",
    "    train_sizes.append(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(12, 8))\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(221+i)\n",
    "    ax.plot(train_sizes[i], train_means[i],\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "    ax.fill_between(train_sizes[i],\n",
    "                 train_means[i] + train_stds[i],\n",
    "                 train_means[i] - train_stds[i],\n",
    "                 alpha=0.15, color='blue')\n",
    "    ax.plot(train_sizes[i], test_means[i],\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "    ax.fill_between(train_sizes[i],\n",
    "                 test_means[i] + test_stds[i],\n",
    "                 test_means[i] - test_stds[i],\n",
    "                 alpha=0.15, color='green')\n",
    "    ax.grid()\n",
    "    ax.set_ylim((0.8,1))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_title(r\"$\\gamma={}, C={}$\".format(*hyperparams[i]), fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity\n",
    "**LinearSVC** utilizza la libreria liblinear che implementa un algoritmo ottimizzato per SVM lineari. La complessita' del training e' $O(mn)$ dove $m$ e' il numero di feature e $n$ il numero di istanze.\n",
    "\n",
    "**SVC** utilizza la libreria libsvm e i tempi di computazione variano $O(m^2n)$ a $O(m^3n)$, rendendo SVM un classificatore per dataset complessi ma di dimensione piccola e media (max 100K istanze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "I decision tree sono un secondo strumento piuttosto versatile per problemi di classificazione e regressione. Inoltre sono una componente fondamentale per Random Forest. \n",
    "Decision Trees can perform both classification and regression. They are a fundamental components of Random Forests. Moreover they do not require feaure scaling.\n",
    "\n",
    "Negli esempi utilizzeremo il dataset Iris per evidenziare le peculiarita' dell'approccio e il dataset delle carte di credito per testare le performance su un caso d'uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "X_iris = iris_data['data'][:,2:]\n",
    "y_iris = iris_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SKL, i decision tree per la classificazione sono implementati dalla classe **DecisionTreeClassifier**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La struttura del decision tree addestrato e' visualizzabile mediante il metodo **export_graphviz()**, il quale crea un file .dot. E' possibile convertire il file .dot in immagine o pdf mediante l'utility Graphviz (https://graphviz.org/download/) e da prompt dei comandi digitare\n",
    "```\n",
    "dot -Tpng <file>.dot -o <file>.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=\"iris_tree.dot\",\n",
    "        feature_names=iris_data.feature_names[2:],\n",
    "        class_names=iris_data.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](iris_tree.png)\n",
    "\n",
    "### Making predictions\n",
    "Uno dei vantaggi di DT e' la leggiblita' del modello (White Box) che indica come il classificatore esegue una predizione. Partendo dalla radice dell'albero, ci chiediamo se la lunghezza del petalo e' minore di 2.45. In caso positivo, ci spostiamo nel figlio sinistro, ed essendo una foglia restituiamo la classe del nodo. Nel caso di petalo con lunghezza maggiore di 2.45, dobbiamo verificare una secondo condizione sulla larghezza del petalo e in base alla misura prediciamo una classe rispetto all'altra.\n",
    "\n",
    "L'attributo **samples** indica a quante istanze del training set si applica la condizione. La somma dei valori samples dei figli deve coincidere con il valore samples del padre. \n",
    "\n",
    "L'attributo **value** indica a quante istanze di una classe, si applica la condizione del nodo. La somma deve corrispondere al valore samples del nodo.\n",
    "\n",
    "L'attributo **gini**  indica il valore di Gini impurity per quel nodo. Se $G = 0$, il nodo e' puro, quindi il nodo si applica ad una sola classe. \n",
    "\n",
    "**Gini impurity**\n",
    "$$G_i = 1 - \\sum_{k=1}^n p_{i,k}^2$$\n",
    "dove $p_{i,k}$ e' il rapporto tra il numero di istanze della classe $k$ in quel nodo e il valore di sample del nodo $i$. Il valore minimo di $G$ e' 0 e si ottiene quando una classe $k$ ha $p_k = 1$. Mentre il valore massimo e' dato da una distribuzione delle classi uniforme, $G = 1 - \\sum_{k=1}^n p_{i,k}^2 = 1 - \\sum_{k=1}^n (1/n)^2 = 1 - \\frac{n}{n^2} = 1 - \\frac{1}{n}$\n",
    "\n",
    "Possiamo anche definire le aree di decisione identificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(tree_clf, X_iris, y_iris)\n",
    "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n",
    "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
    "plt.text(1.40, 1.0, \"Profondita'=0\", fontsize=15)\n",
    "plt.text(3.2, 1.80, \"Profondita'=1\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating class probabilities\n",
    "Un DT stima anche la probabilita' che un'istanza appartenga alla classe $k$. Attraversa l'albero fino a giungere ad una foglia e restituisce la percentuale di elementi con classe $k$ presenti nel nodo. \n",
    "\n",
    "Per esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree_clf.predict_proba([[5, 1.5]]))\n",
    "tree_clf.predict([[5, 1.5]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "49/54, 5/54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART training algorithm\n",
    "SKL utilizza l'algoritmo Classification And Regression Tree (CART) per la fase di training. L'algoritmo prinva divide il training set in due sottoinsiemi basandosi sulla feature $k$ e una soglia $t_k$, individuando la coppia $(k, t_k)$ corrispondente al partizionamento piu' puro. La funzione costo che tenta di minimizzare e'\n",
    "$$J(k,t_k) = \\frac{m_{left}}{m}G_{left}+\\frac{m_{right}}{m}G_{right}$$\n",
    "dove $G_{\\{.\\}}$ misura l'impurita' dei sottoinsiemi di sinistra e destra and $m_{\\{.\\}}$ e' il numero di istanze a sinistra e destra.\n",
    "\n",
    "Lo stesso meccanismo di ricerca della partizione migliore viene applicato ai sottoinsiemi trovati e la procedura prosegue in modo ricorsivo. \n",
    "\n",
    "Il partizionamento si ferma alla profondita' specificata da **max_depth**  o se non si trova una divisione che non riduce l'impurita'\n",
    "\n",
    "**NB:** CART algorithm e' un algortimo greedy - non esegue backtracking. Non garantisce l'ottimalita' delle partizioni.\n",
    "\n",
    "\n",
    "### Entropy or impurity\n",
    "Di default, viene utilizzata la misura di Gini impurity, tuttavia e' possibile utilizzare l'entropia come misuar di impuriat'selezionando il parametro  **criterion** a \"entropy\". L'entropia di una partizione e' 0 quando contiene istanze di una sola classe.\n",
    "$$H_{i} = - \\sum_{k=1}^{n}p_{i,k}log(p_{i,k})$$\n",
    "\n",
    "### Regularization Hyperparameters\n",
    "DT utilizza poche assunzioni sui dati di training. Senza vincoli si adatta al training set e molto probabilmente presentera' il problema dell'overfitting. Per evitare overfitting si possono limitare i gradi di liberta' di un DT agendo sugli iperparametri. I parametri di regolarizzazione dipendono dall'algoritmo utilizzato per il training, tuttavia e' comune agire sulla profondita' massima dell'albero. In SKL la profondita' e' controllata da **max_depth**. Riducendo max_depth riduco la varianza aumentando il bias e riduco il rischio di overfitting.\n",
    "\n",
    "\n",
    "**DecisionTreeClassifier** ha altri parametri che agiscono sulla forma dell'albero da apprendere: \n",
    "- **min_samples_split**: numero minimo istanze affinche' il nodo venga splittato\n",
    "- **min_samples_leaf**: numero minimo di istanze in un nodo foglia,\n",
    "- **min_weight_fraction_leaf**: stessa semantica del parametro precedente ma valutata sulla percentuale rispetto al numero totale di istanze nel training\n",
    "- **max_leaf_nodes**: numero massimo di nodi foglia\n",
    "- **max_features**: numero massimo di feature da valutare nel partizionamento del nodo\n",
    "\n",
    "Aumentare i parametri min_ o ridurre i max_ regolarizza il modello.\n",
    "\n",
    "Vediamo un esempio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n",
    "deep_tree_clf1.fit(X_moon, y_moon)\n",
    "deep_tree_clf2.fit(X_moon, y_moon)\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(deep_tree_clf1, X_moon, y_moon, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n",
    "plt.title(\"No regolarizzazione\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(deep_tree_clf2, X_moon, y_moon, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n",
    "plt.title(\"min_samples_leaf = {}\".format(deep_tree_clf2.min_samples_leaf), fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instability\n",
    "A causa dell'alta varianza i DT sono sensibili a piccole variazioni del training set\n",
    "\n",
    "Rimuovendo il valore piu' elevato di Iris-Versicolor (lunghezza petalo 4.8 cm e larghezza 1.8 cm wide) si ottiene un DT diverso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris[(X_iris[:, 1]==X_iris[:, 1][y_iris==1].max()) & (y_iris==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_widest_versicolor = (X_iris[:, 1]!=1.8) | (y_iris==2)\n",
    "X_tweaked = X_iris[not_widest_versicolor]\n",
    "y_tweaked = y_iris[not_widest_versicolor]\n",
    "print(X_tweaked.shape)\n",
    "tree_clf_tweaked = DecisionTreeClassifier(max_depth=2, random_state=40)\n",
    "tree_clf_tweaked.fit(X_tweaked, y_tweaked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(tree_clf_tweaked, X_tweaked, y_tweaked, legend=False)\n",
    "plt.plot([0, 7.5], [0.8, 0.8], \"k-\", linewidth=2)\n",
    "plt.plot([0, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
    "plt.text(1.0, 0.9, \"Depth=0\", fontsize=15)\n",
    "plt.text(1.0, 1.80, \"Depth=1\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_leaf = [5, 10, 100, 200, 350]\n",
    "\n",
    "train_sizes, train_means, test_means, test_stds, train_stds = [],[],[],[],[]\n",
    "for mlf in min_leaf:\n",
    "    dt_mlf = DecisionTreeClassifier(min_samples_leaf=mlf, random_state=42, max_depth=15)\n",
    "    train_size, train_scores, test_scores = learning_curve(dt_mlf,\n",
    "                                                       X=X,\n",
    "                                                       y=y,\n",
    "                                                       train_sizes=np.linspace(0.1,1.0,10),\n",
    "                                                       cv=10,\n",
    "                                                       n_jobs=-1)\n",
    "    print('fatto {}'.format(mlf))\n",
    "    train_means.append(np.mean(train_scores, axis=1))\n",
    "    train_stds.append(np.std(train_scores, axis=1))\n",
    "    test_means.append(np.mean(test_scores, axis=1))\n",
    "    test_stds.append(np.std(test_scores, axis=1))\n",
    "    train_sizes.append(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(12, 8))\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(231+i)\n",
    "    ax.plot(train_sizes[i], train_means[i],\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "    ax.fill_between(train_sizes[i],\n",
    "                 train_means[i] + train_stds[i],\n",
    "                 train_means[i] - train_stds[i],\n",
    "                 alpha=0.15, color='blue')\n",
    "    ax.plot(train_sizes[i], test_means[i],\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "    ax.fill_between(train_sizes[i],\n",
    "                 test_means[i] + test_stds[i],\n",
    "                 test_means[i] - test_stds[i],\n",
    "                 alpha=0.15, color='green')\n",
    "    ax.grid()\n",
    "    ax.set_ylim((0.8,1))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_title(r\"min_sam_leaf:{}\".format(min_leaf[i]), fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_card_clf = DecisionTreeClassifier(max_depth=15, min_samples_leaf=100, random_state=42)\n",
    "tree_card_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "        tree_card_clf,\n",
    "        out_file=\"card_tree.dot\",\n",
    "        feature_names=columns_name,\n",
    "        class_names=['Not Churn','Churn'],\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spesso se si aggregano e/o combinano le predizioni di un gruppo di classificatori si ottiene un classificatore migliore dei singoli classificatori. Un gruppo di classificatori e' detto **ensemble**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier\n",
    "Un modo per aggregare le predizioni di $n$ classificatori e' restituire la classe piu' votata dai classificatori. Ogni classificatore esegue una predizione e viene restituia la classe di maggioranza: **hard voting**.\n",
    "\n",
    "![](voting_clf.png)\n",
    "\n",
    "Anche se ogni singolo classificatore e' leggermente migliore (weak classifier) di un random guesser (baseline), l'aggregazione determina un classificatore con performance migliori posto che i classificatori deboli siano numerosi e di diverso tipo. Il secondo punto - diversita' di modelli - aumenta l'indipendenza tra i classificatori.\n",
    "\n",
    "In SKL la classe **VotingClassifier** del modulo **ensemble** implementa la strategia di hard voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ensemble_performance(ax, X, y, scores, scoring):\n",
    "    data_score = []\n",
    "    labels = []\n",
    "    for clf in scores['estimator'][0].estimators_:\n",
    "        scores_clf = cross_validate(clf, X, y,\n",
    "                        cv = StratifiedShuffleSplit(n_splits=20, test_size=0.2, random_state=42),\n",
    "                        return_train_score= True,\n",
    "                        scoring = scoring,\n",
    "                        n_jobs=-1)\n",
    "        data_score.extend([scores_clf[t+s] for s in scoring for t in ['train_','test_']])\n",
    "        labels.extend([clf.__class__.__name__+'_'+t+s for s in scoring for t in ['train_','test_']])\n",
    "    data_score.extend([scores[t+s] for s in scoring for t in ['train_','test_']])\n",
    "    labels.extend(['Voting_'+t+s for s in scoring for t in ['train_','test_']])\n",
    "    sns.boxplot(ax = ax,\n",
    "                data = data_score,\n",
    "                whis = [5, 95],\n",
    "                palette = \"vlag\",\n",
    "                orient = 'h'\n",
    "               )\n",
    "    ax.set(yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applichiamo una strategia di hard voting al dataset delle due lune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(random_state=42)\n",
    "svm_clf = SVC(random_state=42)\n",
    "per_clf = Perceptron(random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('pc', per_clf), ('svc', svm_clf)],\n",
    "    voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(voting_clf, X_moon, y_moon,\n",
    "                        cv = StratifiedShuffleSplit(n_splits=20, test_size=0.2, random_state=42),\n",
    "                        return_estimator = True,\n",
    "                        return_train_score= True,\n",
    "                        scoring = ['recall','accuracy','f1'],\n",
    "                        n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot()\n",
    "visualize_ensemble_performance(ax, X_moon, y_moon, scores, ['recall','accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applichiamo la stessa strategia di voting al dataset delle carte di credito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(voting_clf, X, y,\n",
    "                        cv = StratifiedShuffleSplit(n_splits=20, test_size=0.2, random_state=42),\n",
    "                        return_estimator = True,\n",
    "                        return_train_score= True,\n",
    "                        scoring = ['recall','accuracy','f1'],\n",
    "                        n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot()\n",
    "visualize_ensemble_performance(ax, X, y, scores, ['recall','accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tutti i classificatori stimano la probabilita' che un'istanza appartenga ad una classe - implementano il metodo **predict_proba**, allora si puo' applicare una strategia di _soft voting_. Viene predetta la classe con lo score medio piu' elevato, dove la media si valuta sugli score restituiti dai singoli classificatori per una determinata classe. In generale, la strategia di soft voting restituisce risultati migliori rispetto ad hard voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and pasting\n",
    "Un secondo approccio di ensemble e' utilizzare lo stesso modello di apprendimento ma addestrato su diversi partizionamenti del training set. Se utilizzo un campionamento con reimmissione, il metodo e' detto **bagging** o **bootstrap aggregating**, mentre in caso di non reimmisione, il metodo e' detto **pasting**.\n",
    "\n",
    "![](bagging.png)\n",
    "\n",
    "Per effettuare una predizione si aggregano le predizioni dei classificatori usando la _moda_ (classificazione) oppure la media (regressione). In generale, il predittore ensemble ha una varianza minore rispetto al singolo predittore addestrato su tutto il dataset.\n",
    "\n",
    "In SKL bagging e pasting sono implementati dalla classe **BaggingClassifier** del modulo **ensemble**. Il passaggio da un metodo all'altro e' gestito dal parametro **boostrap**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=500, max_samples=200, bootstrap=True, n_jobs=-1)\n",
    "dt_clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_means, test_means, test_stds, train_stds = [],[],[],[],[]\n",
    "for clf in [bag_clf, dt_clf]:\n",
    "    train_size, train_scores, test_scores = learning_curve(clf,\n",
    "                                                       X=X_moon,\n",
    "                                                       y=y_moon,\n",
    "                                                       train_sizes=np.linspace(0.1,1.0,10),\n",
    "                                                       cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42),\n",
    "                                                       n_jobs=-1)\n",
    "    print('fatto {}'.format(clf))\n",
    "    train_means.append(np.mean(train_scores, axis=1))\n",
    "    train_stds.append(np.std(train_scores, axis=1))\n",
    "    test_means.append(np.mean(test_scores, axis=1))\n",
    "    test_stds.append(np.std(test_scores, axis=1))\n",
    "    train_sizes.append(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(12, 8))\n",
    "for i in range(2):\n",
    "    ax = fig.add_subplot(121+i)\n",
    "    ax.plot(train_sizes[i], train_means[i],\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "    ax.fill_between(train_sizes[i],\n",
    "                 train_means[i] + train_stds[i],\n",
    "                 train_means[i] - train_stds[i],\n",
    "                 alpha=0.15, color='blue')\n",
    "    ax.plot(train_sizes[i], test_means[i],\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "    ax.fill_between(train_sizes[i],\n",
    "                 test_means[i] + test_stds[i],\n",
    "                 test_means[i] - test_stds[i],\n",
    "                 alpha=0.15, color='green')\n",
    "    ax.grid()\n",
    "    ax.set_ylim((0.8,1))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_means, test_means, test_stds, train_stds = [],[],[],[],[]\n",
    "for clf in [bag_clf, dt_clf]:\n",
    "    train_size, train_scores, test_scores = learning_curve(clf,\n",
    "                                                       X=X,\n",
    "                                                       y=y,\n",
    "                                                       train_sizes=np.linspace(0.1,1.0,10),\n",
    "                                                       cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42),\n",
    "                                                       n_jobs=-1)\n",
    "    print('fatto {}'.format(clf))\n",
    "    train_means.append(np.mean(train_scores, axis=1))\n",
    "    train_stds.append(np.std(train_scores, axis=1))\n",
    "    test_means.append(np.mean(test_scores, axis=1))\n",
    "    test_stds.append(np.std(test_scores, axis=1))\n",
    "    train_sizes.append(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(12, 8))\n",
    "for i in range(2):\n",
    "    ax = fig.add_subplot(121+i)\n",
    "    ax.plot(train_sizes[i], train_means[i],\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "    ax.fill_between(train_sizes[i],\n",
    "                 train_means[i] + train_stds[i],\n",
    "                 train_means[i] - train_stds[i],\n",
    "                 alpha=0.15, color='blue')\n",
    "    ax.plot(train_sizes[i], test_means[i],\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "    ax.fill_between(train_sizes[i],\n",
    "                 test_means[i] + test_stds[i],\n",
    "                 test_means[i] - test_stds[i],\n",
    "                 alpha=0.15, color='green')\n",
    "    ax.grid()\n",
    "    ax.set_ylim((0.8,1))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaggingClassifier supporta il campionamento delle feature anziche' delle istanze, mediante i parametri **max_features** e **bootstrap_features**. In questo modo ogni classificatore utilizza un sottoinsieme di feature. Se campiono sia feature sia istanze applico un metodo **Random Patches**, mentre se campiono solo le feature applico un metodo di **Random Subspaces**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "RF e' un ensemble di Decision Trees utilizzando un metodo di bagging con *max_samples* posto uguale alla numerosita' del training set. In SKL e' implementato dalla classe **RandomForestClassifier** ottimizata per i DT rispetto all'applicazione della classe BaggingClassifier. Coniugando l'approccio bagging e DT, gli iperparametri dell'algoritmo sono l'unione degli iperparametri degli elementi costitutivi. Inoltre, nella crescita dell'albero non vengono considerate tutte le feature per cercare lo splitting migliore ma un sottoinsieme casuale. In questo modo aumento la diversita' degli alberi.\n",
    "\n",
    "Inoltre, riguardando la formulazione dell'algoritmo CART, il partizionamento e' basato anche il parametro di soglia $t_k$, il quale puo' essere invece inizializzato in maniera casuale per ogni feature. Una foresta di questo tipo e' detta **Extremely Randomized Trees** or **Extra-tree**. Il vanataggio di tale approccio sono le migliori prestazioni computazionali nella fase di training. In SKL la classe **ExtraTreeClassifier** implementa l'algoritmo extra-tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train[y_train == 1])/len(y_train), len(y_test[y_test == 1])/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnf_clf = RandomForestClassifier(n_estimators=250, max_leaf_nodes=64, n_jobs=-1, max_features=10)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=250, max_leaf_nodes=64, n_jobs=-1, max_features=10)\n",
    "scores_rnf = cross_val_score(rnf_clf, X_train, y_train, cv=5, scoring='f1', n_jobs=-1)\n",
    "scores_et = cross_val_score(et_clf, X_train, y_train, cv=5, scoring='f1',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot()\n",
    "sns.boxplot(ax = ax,\n",
    "            data = [scores_rnf, scores_et],\n",
    "            palette = 'vlag',\n",
    "            orient = 'h'\n",
    "           )\n",
    "ax.set(yticklabels=['RF','ET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In un singolo DT le feature piu' importanti, i.e. discriminanti, molto probabilmente sono posizionate nei nodi prossimi alla radice, mentre quelle meno importanti nei nodi vicino alle foglie. E' possibile avere una stima dell'importanza di una feature calcolando la profondita' media in cui una feature appare negli alberi facenti parte della foresta.\n",
    "\n",
    "In SKL l'importanza delle feature e' disponibile accedendo all'attributo **feature_importances_**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnf_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_feat_importance = dict(zip(columns_name, rnf_clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(named_feat_importance.items(), key=lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo modo ottengo in modo rapido una prima evidenza sull'importanza delle feature ed eventualmente sulla possibilita' di eseguire una feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Sono metodi di ensemble in cui i classificatori sono addestrati in modo sequenziale, ed ogni classificatori corregge il classificatore precedente. I metodi piu' importanti sono *AdaBoost* e *Gradient Boosting*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "Un modo per correggere un classificatore e' focalizzari sulle istanze che il predecessore non classifica correttamente, cioe' sulla istanze piu' difficili.\n",
    "\n",
    "![](adaboost.png)\n",
    "\n",
    "Per dare piu' importanza alle istanze non classificate correttamente aumento il peso di quelle istanze e addestro un classificatore con le istanze ri-pesate. Il processo viene ripetuto per il numero di classificatori nel pool.\n",
    "\n",
    "In AdaBoost il peso associato ad ogni istanza e' $w_i=1/m$. addestrato il primo classificatore posso calcolare il tasso di errore pesato $r_j$ ($j=1$):\n",
    "$$r_j = \\frac{\\sum_{i=1, \\hat{y}_j(i)\\neq y(i)}^m w_i}{\\sum_{i=1}^m w_i}$$\n",
    "dove $\\hat{y}_j(i)$ indica la predizione del classificatore $j$ sull'istanza $i$.\n",
    "\n",
    "Il peso del classificatore  $j$ viene calcolato come:\n",
    "$$\\alpha_j = \\eta log \\frac{1-r_j}{r_j}$$\n",
    "dove $\\eta$ e' un iperparametro e $\\alpha_j$ ha la forma\n",
    "\n",
    "![](weight_boost.png)\n",
    "\n",
    "Infine i pesi delle istanze sono aggiornati secondo la seguente equazione:\n",
    "\n",
    "$$\n",
    "    w_i \\leftarrow \n",
    "\\begin{cases}\n",
    "    w_i & \\text{se } \\hat{y}_j(i) = y(i)\\\\\n",
    "    w_i exp(\\alpha_j),              & \\text{altrimenti}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "normalizzando poi i pesi.\n",
    "\n",
    "Dopo l'aggiornamento dei pesi il classificatore $j+1$ viene addestrato utilizzando i pesi aggiornati.\n",
    "\n",
    "La classe predetta e' quella che riceve la maggioranza dei voti pesati:\n",
    "$$\\hat{y}(x) = argmax_k \\sum_{j=1, \\hat{y}_j=k} ^N  \\alpha_j$$\n",
    "\n",
    "In SKL il metodo AdaBoost e' implementato dalla classe **AdaBoostClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo osservare l'effetto del parametro $\\eta$ in figura.\n",
    "\n",
    "![](rate_boost.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=6),\n",
    "    n_estimators=100,\n",
    "    algorithm='SAMME.R',\n",
    "    learning_rate=0.5\n",
    ")\n",
    "scores_ada = cross_val_score(ada_clf, X_train, y_train, cv=5, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot()\n",
    "sns.boxplot(ax = ax,\n",
    "            data = [scores_rnf, scores_et, scores_ada],\n",
    "            palette = 'vlag',\n",
    "            orient = 'h'\n",
    "           )\n",
    "ax.set(yticklabels=['RF','ET','ADA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Similmente ad AdaBoost, Gradient Boosting agisce in maniera sequenziale ma in ogni step il classificatore apprende sugli errori residui del classificatore precedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_rnd = np.random.rand(100, 1) - 0.5\n",
    "y_rnd = 3*X_rnd[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X_rnd, y_rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y_rnd - tree_reg1.predict(X_rnd)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X_rnd, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2 - tree_reg2.predict(X_rnd)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X_rnd, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.axis(axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,11))\n",
    "\n",
    "plt.subplot(321)\n",
    "plot_predictions([tree_reg1], X_rnd, y_rnd, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Residui e DT\", fontsize=16)\n",
    "\n",
    "plt.subplot(322)\n",
    "plot_predictions([tree_reg1], X_rnd, y_rnd, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Predizione Gradient Boosting\", fontsize=16)\n",
    "\n",
    "plt.subplot(323)\n",
    "plot_predictions([tree_reg2], X_rnd, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n",
    "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n",
    "\n",
    "plt.subplot(324)\n",
    "plot_predictions([tree_reg1, tree_reg2], X_rnd, y_rnd, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.subplot(325)\n",
    "plot_predictions([tree_reg3], X_rnd, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n",
    "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.subplot(326)\n",
    "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X_rnd, y_rnd, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "Gli iperparametri sono parametri non appresi direttamente dal classificatore. In SKL corrispondono ai parametri passati al costruttore della classe che implementa il classificatore. Ogni parametro puo' essere ottimizzato.\n",
    "\n",
    "Il processo di ottimizzazione richiede:\n",
    "\n",
    "A search object consists of:\n",
    "- un oggetto Estimator;\n",
    "- uno spazio dei parametri;\n",
    "- un metodo di ricerca o campionamento dei candidati;\n",
    "- uno schema di cross-validation scheme; e\n",
    "- una funzione di score.\n",
    "\n",
    "In SKL sono disponibili due approcci per la ricerca o il campionamento dello spazio dei parametri: \n",
    "- **GridSearchCV** che considera in modo esaustivo tutte le combinazioni di parametri\n",
    "- **RandomizedSearchCV** campiona un numero dato di parametri da una distribuzione sullo spazio dei parametri."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "Grid search implementata in **GridSearchCV** genera gli insiemi di parametri da una griglia di parametri specificati dal parametro **param_grid**. GridSearchCV implementa l'interfaccia Estimator, in cui il metodo fit valuta tutte le combinazioni ammissibili e identifica la combinazione migliore.\n",
    "\n",
    "Iniziamo ad valutare solo un insieme di classificatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'kernel': ['rbf'], 'gamma': [.1, 5, 10], 'C': [0.1, 1, 1000]},\n",
    "    {'kernel': ['poly'], 'degree': [1, 2, 3], 'coef0': [1, 10, 50]}\n",
    "]\n",
    "\n",
    "svm_clf = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_clf,\n",
    "                           param_grid = param_grid,\n",
    "                           cv = 5,\n",
    "                           scoring = 'f1',\n",
    "                           n_jobs = -1\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il precedente codice ci impiega circa 2/3 minuit su 12 core\n",
    "\n",
    "Mediante il metodo **best_params_** si ottiene la combinazione di parametri migliore - in questo caso secondo lo score $F1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante i metodi **best_estimator_** and **best_score_** si possono ottenere il migliore classificatore e il relativo score. Di default, la classe esegue un refit su tutto il dataset - **refit=True** - utilizzando i parametri ottimali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best: {}'.format(grid_search.best_score_))\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutti gli score per ogni combinazione di parametri sono ottenibili mediante il metodo **cv_results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)[['mean_test_score','params']]\n",
    "results.sort_values(by='mean_test_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedGridSearch\n",
    "\n",
    "In RGS ogni insieme di parametri e' campionato da una distribuzione sullo spazio dei parametri. Il metodo di estrazione di ogni parametro e' specificato mediante un dictionary a cui si aggiunge un budget indicante quanti insiemi di parametri devo generare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    'kernel': ['poly'], \n",
    "    'degree': randint(2,8),\n",
    "    'coef0': [1, 10, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(estimator=svm_clf,\n",
    "                                   param_distributions= param_dict,\n",
    "                                   cv = 5,\n",
    "                                   scoring = 'f1',\n",
    "                                   n_iter=5,\n",
    "                                   n_jobs = -1\n",
    "                                  )\n",
    "\n",
    "random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(random_search.cv_results_)[['mean_test_score','params']]\n",
    "results.sort_values(by='mean_test_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCv e RandomizedSearchCV permettono la ricerca dei parametri ottimali su estimator composti come Pipeline e ColumnTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_data = pd.read_csv('BankChurnersMissingData.csv')\n",
    "credit_card_data.dropna(subset=['Total_Revolving_Bal','Months_Inactive_12_mon'],\n",
    "                   inplace=True\n",
    "                  )\n",
    "\n",
    "# Estraggo la  colonna delle label e la rimuovo dal dataset\n",
    "credit_card_label = credit_card_data['Attrition_Flag'].map(\n",
    "    {'Existing Customer':0,\n",
    "     'Attrited Customer':1\n",
    "    }\n",
    ").values\n",
    "credit_card_data.drop(columns=['Attrition_Flag',\n",
    "                               'CLIENTNUM',\n",
    "                               'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n",
    "                               'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1'],\n",
    "                      inplace=True)\n",
    "def unknown_imputer(X, missing_value = 'Unknown'):\n",
    "    X = X.values\n",
    "    unique_values, count = np.unique(X,return_counts=True)\n",
    "    num_nan = count[unique_values == missing_value]\n",
    "    counting = count[unique_values != missing_value]\n",
    "    values = unique_values[unique_values != missing_value]\n",
    "    X_new = X.copy()\n",
    "    freq = counting / np.sum(counting)\n",
    "    X_new[X_new == missing_value] = np.random.choice(values,size=num_nan,p=freq)\n",
    "    return X_new\n",
    "\n",
    "ui = FunctionTransformer(unknown_imputer)\n",
    "\n",
    "customer_age_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "category_pipeline = Pipeline([\n",
    "    ('imputer', FunctionTransformer(unknown_imputer)),\n",
    "    ('ordinal', OneHotEncoder())\n",
    "])\n",
    "\n",
    "features_robust = ['Credit_Limit','Total_Revolving_Bal','Avg_Open_To_Buy']\n",
    "features_standard = list(set(credit_card_data.select_dtypes(include=['int64','float64']).columns).difference(set(features_robust + ['Avg_Utilization_Ratio', 'Customer_Age'])))\n",
    "\n",
    "\n",
    "data_preprocessing = ColumnTransformer([\n",
    "    ('age', customer_age_pipeline, ['Customer_Age']),\n",
    "    ('gender', OrdinalEncoder(categories=[['M','F']]), ['Gender']),\n",
    "    ('edu', category_pipeline, ['Education_Level']),\n",
    "    ('status', category_pipeline, ['Marital_Status']),\n",
    "    ('income', category_pipeline, ['Income_Category']),\n",
    "    ('card', category_pipeline, ['Card_Category']),\n",
    "    ('numeric_robust', RobustScaler(), features_robust),\n",
    "    ('feature_standard', StandardScaler(), features_standard)\n",
    "],\n",
    "    remainder = 'passthrough'\n",
    ")\n",
    "\n",
    "clf_forest_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', data_preprocessing),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_forest_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'preprocessing__age__imputer__strategy': ['most_frequent','median'],\n",
    "     'classifier__max_features': [8,10,'auto'],\n",
    "     'classifier__max_depth':[2,5,8,None],\n",
    "     'classifier__n_estimators':[10,20,50,100,150]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=clf_forest_pipeline,\n",
    "                           param_grid = param_grid,\n",
    "                           cv = 3,\n",
    "                           scoring = 'f1',\n",
    "                           n_jobs = -1\n",
    ")\n",
    "\n",
    "grid_search.fit(credit_card_data, credit_card_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso valutare anche diverse metriche nella stessa valutazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(clf_forest_pipeline, \n",
    "                           param_grid= param_grid,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1,\n",
    "                           scoring=['f1','accuracy'],\n",
    "                           refit = 'f1'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(credit_card_data, credit_card_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)[['mean_test_f1','params','mean_test_accuracy']]\n",
    "results.sort_values(by='mean_test_f1',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced class\n",
    "Una strategia per gestire lo sbilanciamento delle classi e' il campionamento o resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fake, y_fake = make_classification(\n",
    "    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n",
    "    n_informative=3, n_redundant=1, flip_y=0,\n",
    "    n_features=20, n_clusters_per_class=1,\n",
    "    n_samples=1000, random_state=10\n",
    ")\n",
    "\n",
    "df_fake = pd.DataFrame(X_fake)\n",
    "df_fake['target'] = y_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.target.value_counts().plot(kind='bar', title='Count (target)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per trattare le classi sbilanciate dobbiamo installare il package imbalanced-learn: \n",
    "```\n",
    "pip install -U imbalanced-learn\n",
    "```\n",
    "Le tecniche di resampling sono divise in:\n",
    "- upsampling della classe di minoranza\n",
    "- downsampling della classe di maggioranza\n",
    "- generazione di istanze sintetiche\n",
    "\n",
    "### Upsampling\n",
    "Possiamo fare upsampling della classi minoritaria mediante la classe **RandomOverSampler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_sample(X_fake, y_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_ros, return_counts=True)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "Per sottocampionare - downsampling - la classe maggioritaria possiamo  utilizzare:\n",
    "- **RandomUnderSampler**\n",
    "- **TomenLinks** per un downsampling targettizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_rus, y_rus = rus.fit_sample(X_fake, y_fake)\n",
    "np.unique(y_rus, return_counts=True)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Tomek links sonon coppie di istanze molto vicine ma di classi opposte. Rimuovendo le istanze della classe maggioritaria si accresce lo spazio tra le classi\n",
    "![](tomek.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks(sampling_strategy='majority')\n",
    "X_tl, y_tl = tl.fit_sample(X_fake, y_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample generation\n",
    "SMOTE (Synthetic Minority Oversampling TEchnique) genera elementi sintetici della classe minoritaria, basandosi su qulli che esistono gia'. Scegliendo in modo casuale un punto della classe minoritaria, ricerca i k punti della stessa classe piu' vicini - k-nearest neighbors - e genera un punto tra gli estremi identificati.\n",
    "\n",
    "![](smote.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_sm, y_sm = smote.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il precedente processo puo' essere inserito in una pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = mp(\n",
    "    SMOTE(),\n",
    "    SVC()\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'smote__sampling_strategy' : [0.3, 0.5, 0.7, 'minority'],\n",
    "    'svc__kernel' : ['rbf'],\n",
    "    'svc__gamma' : [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe,\n",
    "                          param_grid= param_grid,\n",
    "                          cv = 5,\n",
    "                          n_jobs=-1,\n",
    "                          scoring = 'roc_auc')\n",
    "grid_search.fit(X_fake,y_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel caso non si volesse ricampionare i dati, si possono incorporare i pesi delle classi nella funzione di costo, assegnando alla classe di minoranza un peso maggiore \n",
    "\n",
    "Scikit-learn SKL fornisce una funzione per calcolare i pesi in base alla distribuzione delle classi\n",
    "```python\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "weights = compute_class_weight('balanced', np.unique(y_fake), y_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_w = RandomForestClassifier(class_weight={i:e for i, e in enumerate(weights)})\n",
    "rf_w.fit(X_fake,y_fake)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
