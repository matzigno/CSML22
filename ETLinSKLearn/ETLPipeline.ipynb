{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data management\n",
    "import pandas as pd\n",
    "\n",
    "# Data preprocessing and trasformation (ETL)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, FunctionTransformer, Binarizer, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Math and Stat modules\n",
    "import numpy as np\n",
    "from scipy.special import logit, expit\n",
    "from scipy import stats\n",
    "\n",
    "#Supervised Learning\n",
    "\n",
    "# Unsupervised Learning\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From getting to pre-processing data\n",
    "In questa notebook affronteremo alcuni elementi che fanno parte di un progetto orientato al Machine Learning:\n",
    "1. Ottenere i dati (facilitato)\n",
    "2. Visualizzare i dati ottenuti\n",
    "3. Trasformare/ manipolare i dati per fornire un input agli algoritmi di ML\n",
    "\n",
    "A fine didattici/progetti personali, le fonti per ottenere dataset open sono molteplici:\n",
    "- Kaggle (https://www.kaggle.com/)\n",
    "- Machine Learning repository @ UC Irvine (https://archive.ics.uci.edu/ml/index.php)\n",
    "- Reddit: subreddit dataset (https://www.reddit.com/r/datasets/)\n",
    "\n",
    "Per questa prima esperienza utilizziamo un dataset ottenuto da Kaggle che affronta il problema dall **churn prediction**. Possiamo sintetizzare il problema in questo modo: *dato un insieme di clienti che hanno sottoscritto un servizio e caratterizzati/descritti da un insieme di proprieta', identificare un sottoinsieme di client che molto probabilmente abbondonera' in un dato momento nel futuro (di solito a breve termine) il servizio*. Il problema e' piuttosto generico in quanto riguarda in sostanza la risoluzione di un contratto.\n",
    "\n",
    "<p style=\"border: 3px solid crimson;box-shadow: 5px 6px indianRed;padding:20px;\">Formalmente ogni cliente $c$ e' descritto da un vettore $\\mathbf{c}=<c_1,\\ldots,c_n>$ t.c. $c_i\\in S_i$, dove $S_i$ e' un insieme, denota la proprieta' $i$-esima che caratterizza l'utente $c$. Per predirre qualora un generico utente con determinate proprieta' al tempo $t$, al tempo $t+\\Delta t$ abbandoni il servizio, possiamo **imparare** (learning) in modo automatico (machine) una funzione $f:S_1\\times\\ldots\\times S_n\\rightarrow \\{0,1\\}$, dove $0$ indica la permanenza nel servizio, viceversa $1$ indica l'abbandono del servizio.</p>\n",
    "\n",
    "Nel nostro caso affrontiamo il problema di churn prediction in ambito bancario: credit card churn prediction. \n",
    "\n",
    "Il dataset e' disponibile all'indirizzo https://www.kaggle.com/sakshigoyal7/credit-card-customers e riporta la seguente descrizione:\n",
    "```\n",
    "A manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction.\n",
    "This dataset consists of 10,000 customers mentioning their age, salary, marital_status, credit card limit, credit card category, etc. There are nearly 18 features.\n",
    "We have only 16.07% of customers who have churned. Thus, it's a bit difficult to train our model to predict churning customers.\n",
    "```\n",
    "\n",
    "### Definire l'obiettivo\n",
    "Lo sviluppo di un progetto che utilizzi ML solitamente e' orientato ad un **obiettivo** che non riguarda propriamente il ML, ma che utilizza le metodologie ML per **raggiungere** un'obiettivo. L'obiettivo determina:\n",
    "- come modello il problema\n",
    "    - supervisionato/non supervisionato, classificazione/regressione, batch/online, univariata/multivariata\n",
    "- quali modelli/algoritmi adottare\n",
    "    - fortemente dipendente dal punto precedente\n",
    "- quali misure di performance utilizzare nella valutazione del modello\n",
    "\n",
    "Nel nostro caso (credi card churn prediction) l'obiettivo e'molto specifico e ben definito, e la precedente trattazione ci porta a modellare il problema come un problema _supervisionato_ di _classificazione_ binaria in modalita' batch dato che il dataset e' facilmente caricabile in memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and loading the data\n",
    "\n",
    "Per un'analisi preliminare del dataset e caricarlo in memoria utilizziamo il modulo **pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv('BankChurners.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Commento_: Potevo avere un'anteprima del file utilizzando anche un foglio di calcolo, dato che il formato e' CSV. E' utile verificare se la prima riga del file sia l'intestazione o header, e quindi contenere il nome delle colonne. Per verificare la presenza dello header posso utilizzare il comando _head_ della shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere una prima intuizione sul tipo di dati messi a disposizione, possiamo visualizzare le prime 5 righe del dataset utilizzando il metodo **head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci sono 23 colonne.\n",
    "\n",
    "Per avere una piu'completa descrizione della struttura del dataset utilizziamo i metodi **info** oppure **describe**. Il primo metodo e' piu'orientato alla rappresentazione del dato, mentre il secondo metodo riporta le statistiche riguardanti le varie colonne, qualora siano di tipo numerico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confermiamo che abbiamo dati riguardanti 10127 clienti e in questo caso non abbiamo dati mancanti (Non-null count). I tipi di dati sono sia numerici sia categorici (Dtype).\n",
    "\n",
    "**NB**: A seguire il dataset originale viene corrotto inserendo dati mancanti. Il dataset corrotto e' contenuto nel file _BankChurnersMissingData.csv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere anche una prima impressione sulle colonne di tipo categorico/testuale possiamo utilizzare il metodo **value_counts**.\n",
    "\n",
    "ðŸ¤¡ Su diverse microblogging platforms trovare numerosi thread sulla controintuivita' del nome di questo metodo. SPOILER: la probabilita' di scriverlo correttamente al primo colpo e' prossima allo zero ðŸ¤¡."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine otteniamo il nome delle varie colonne in modo da inferire - qualora non sia specificato in qualche documento - il significato della colonna.<br>\n",
    "Tramite l'attributo **columns** otteniamo questa informazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Nome colonna | Significato|\n",
    "| --- | --- |\n",
    "|CLIENTNUM| ID del cliente|\n",
    "|Attrition_Flag| Indica se il cliente ha abbandonato (valore 1)|\n",
    "|Customer_Age| Eta' del cliente in anni|\n",
    "|Gender|Genere del cliente|\n",
    "|Dependent_count| Numero di dipendenti|\n",
    "|Education_Level| Livello di istruzione del cliente|\n",
    "|Marital_Status| Stato civile|\n",
    "|Income_Category| Livello di reddito in dollari|\n",
    "|Card_Category| Tipo di carta di credito|\n",
    "|Months_on_book| Numero di mesi di iscrizione al servizio|\n",
    "|Total_Relationship_Count| Numero di prodotti in possesso del cliente|\n",
    "|Months_Inactive_12_mon| Numero di mesi di inattivita' negli ultimi 12 mesi|\n",
    "|Contacts_Count_12_mon| Numero di contatti negli ultimi 12 mesi|\n",
    "|Credit_Limit| Limite di credito sulla carta|\n",
    "|Total_Revolving_Bal| Bilancio revolving|\n",
    "|Avg_Open_To_Buy|\n",
    "|Total_Amt_Chng_Q4_Q1| Differenza del totale delle transazioni tra Q4 e Q1|\n",
    "|Total_Trans_Amt| Totale delle transazioni su ultimi 12 mesi|\n",
    "|Total_Trans_Ct| Numero di transazioni negli ultimi 12 mesi|\n",
    "|Total_Ct_Chng_Q4_Q1| Differenza del numero di transazioni in Q4 rispetto a Q1|\n",
    "|Avg_Utilization_Ratio| ?|\n",
    "|Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1| Da eliminare|\n",
    "|Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2| Da eliminare|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un'analisi preliminare delle caratteristiche del dataset puo' avvenire in modo _qualitativo_ attraverso la **visualizzazione** della **distribuzione delle proprieta'**. \n",
    "\n",
    "Utilizziamo il modulo **matplotlib** per la visualizzazione nel corso di questo laboratorio, insieme al modulo **seaborn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas il metodo **hist** permette una rapida visualizzazione della distribuzione delle proprieta' numeriche attraverso istogrammi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prima di visualizzare le distribuzioni, rimuoviamo le colonne che iniziano con Naive Bayes e la colonna con il codice cliente\n",
    "raw_dataset.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n",
    "                  'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n",
    "                  'CLIENTNUM'],\n",
    "                 axis=1,\n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.hist(figsize=(22,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal momento che il dataset non e' troppo grande, possiamo valutare il coefficiente di correlazione di Pearson mediante il metodo **corr**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_correlazione = raw_dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_correlazione['Customer_Age'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificando il parametro _method_ posso cambiare il tipo di correlazione, verificando l'esistenza di una correlazione monotonica, non necessariamente lineare tra le proprieta'. Nel caso in analisi utilizziamo il coefficiente di rank correlation di Spearman (Kendall rank correlation e' altra opzione)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_ranking_correlazione = raw_dataset.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_ranking_correlazione['Customer_Age'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma il risultato non cambia :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the \"correlation\"\n",
    "Un'analisi qualitativa della correlazione e' effettuabile attraverso la visualizzazione delle **distribuzioni congiunte** - a coppie - delle proprieta' di tipo numerico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scegliamo un sottoinsieme di caratteristiche che hanno un diverso livello di correlazione con l'eta' del cliente e visualizziamo una grigia che contiene gli scatter plot di coppie di proprieta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonne_interesse = ['Customer_Age', 'Months_on_book', 'Credit_Limit', 'Total_Trans_Amt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(raw_dataset[colonne_interesse], figsize=(22,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per aumentare la leggibilita' e' possibile specificare la coppia di proprieta' di cui analizzare la distribuzione congiunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.plot(kind='scatter', x = 'Customer_Age', y='Total_Trans_Ct', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.plot(kind='scatter', x = 'Customer_Age', y='Months_on_book', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa fase di esplorazionee, posso definire delle proprieta' che **combinano** le proprieta' iniziali.\n",
    "\n",
    "Per esempio definiamo l'ammontare medio di una transazione negli ultimi ultimi 12 mesi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['Avg_transaction'] = raw_dataset['Total_Trans_Amt']/raw_dataset['Total_Trans_Ct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. e ne visualizziamo la distribuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['Avg_transaction'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for Machine Learning algorithms\n",
    "In questa sezione prepariamo il nostro dataset per essere utilizzato da un modello di ML - nel nostro caso un classificatore binario.\n",
    "\n",
    "**NB**: Prima di procedere, dobbiamo caricare un versione modificata del dataset orginale in cui sono state cancellate delle entry relative ad alcuni clienti e alcune collane. Tale modifica e' stata resa necessaria per introdurre alcuni step di data cleaning/data preparation, che altrimenti non si sarebbero potuti applicare.\n",
    "\n",
    "Carichiamo la nuova versione del dataset ed analizziamo quali feature sono state corrotte e/o contenenti dati mancanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('BankChurnersMissingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le colonne 22 e 23 contengono molte entry con dati mancanti, mentre abbiamo dati mancanti sporadici nelle colonne Customer_Age, Months_Inactive_12_mon, Total_Revolving_Bal.\n",
    "\n",
    "Molti algoritmi di ML non gestiscono i **dati mancanti**, di conseguenza una fase della preparazione dei dati e' la definizione di funzioni che gestiscono i dati mancanti. Le opzioni piu' utilizzate sono 3:\n",
    "- rimuovo il sample che contiene le entry corrotte/mancanti => cancellazione della riga\n",
    "- rimuovo l'intera feature/colonna contenente i dati mancanti/corrotti => rimozione della colonna\n",
    "- imputo/inferisco i dati mancanti/corrotti mediante diverse strategie ben consolidate oppure originali\n",
    "\n",
    "Prima di gestire i dati mancanti, vediamo come **identificare** alcuni sample che contengono un dato mancante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_missing_credit = credit_data[credit_data['Customer_Age'].isnull()]\n",
    "sample_missing_credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating samples with missing values\n",
    "Andiamo ad eliminare la riga relativa al cliente con un valore mancante nella colonna 'Total Revolving_Bal'. In questo caso non eseguiamo nessuna imputazione dal momento che togliamo un solo campione/sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.dropna(subset=['Total_Revolving_Bal'],\n",
    "                   inplace=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho eliminato solo una riga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating features with missing values\n",
    "Il dataset contiene due colonne che:\n",
    "1) contengono molti dati mancanti\n",
    "2) sono inutili al fine della classificazione in quanto contengono la predizione di un classificatore\n",
    "\n",
    "In questo caso possiamo tranquillamente eliminare le colonne. In generale, tutttavia, l'eliminazione di una feature deve seguire da una scelta ponderata sulle rimanenti opzioni per la gestione dei dati mancanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n",
    "                    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'],\n",
    "                   inplace=True,\n",
    "                   axis=1\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing values\n",
    "Abbiamo ancora una colonna che contiene 6 dati mancanti. In questo caso possiamo inferire il valore dei dati mancanti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values in Pandas\n",
    "Gestiamo l'imputazione del dato mancante in Pandas, mediante alcune statistiche e il metodo **fillna**. Ci concentriamo sulla colonna 'Customer_Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = credit_data['Customer_Age'].median()\n",
    "median_age\n",
    "# credit_data['CustomerAge'].fillna(median_age, inplace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values in Scikit-Learn\n",
    "Scikit-learn - SKL - offre una soluzione piu' completa per gestire l'imputazione dei dati mancanti.\n",
    "\n",
    "La classe **SimpleImputer** fornisce una strategia base. I valori mancanti possono essere imputati da valori costanti oppure statistiche puntuali.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(credit_data['Customer_Age'].values.reshape(-1, 1))\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo fit non esegue alcuna sostituzione ma esegue solamente la strategia utilizzata sul vettore o matrice che ricve in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['Customer_Age'] = imputer.transform(credit_data['Customer_Age'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo transform applica la strategia al vettore/matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn API e' organizzato secondo una serie di principi che ne ispirano il design e l'architettura:\n",
    "\n",
    "1) **Consistency**: ogni oggetto condivide un'interfaccia consistente e condivisa\n",
    "    - **Estimator**: un qualsiasi oggetto che stima alcuni parametri. La stima e' eseguita dal metodo **fit** che richiede un dataset come parametro.\n",
    "    - **Transformers**: un estimator che trasforma un dataset. La trasformazione e' eseguita dal metodo **transform** che richiede il dataset da trasformare come parametro. Restituisce il dataset trasformato. Esiste un metodo **fit_trasform** che esegue entrambi i metodi ed e'ottimizzato.\n",
    "    - **Predictors**: un oggetto estimator che predice. Un predictor ha il metodo **predict** che richiede un insieme di sample nuovi e restituisce le predizioni associate.\n",
    "2) **Inspection**: tutti gli iperparametri sono accessibili mediante attributi, cosi' come i parametri appresi. Si utilizza la notazione con underscore (vedi statistics_ in SimpleImputer)\n",
    "3) **Nonproliferation of classes**: datasets sono Numpy arrays o matrici sparse Scipy. Non ci sono oggetti dataset definiti da utenti\n",
    "4) **Composition**: i moduli/classi rilasciati sono componibili\n",
    "5) **Sensible defaults**: si devono definire dei valori di default ragionevoli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo un ulteriore problema costituito dalla colonna 'Income_Category'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['Income_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una possibile soluzione e' quella di inferire i valori mancanti sulla base delle feature. In questo caso dobbiamo risolvere un problema di classificazione/regressione utilizzando le entry con i valori non mancanti come label. SKL mettere a disposizione la classe sperimentale **IterativeImputer** (https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer), tuttavia non e' possibile utilizzarlo con in valori categorici che abbiamo. Infatti, se eseguite le due celle seguenti ottenete un errore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_it = IterativeImputer(max_iter=10, random_state=0)\n",
    "imputer_it.fit(credit_data[['Gender','Income_Category']].values)\n",
    "IterativeImputer(random_state=0)\n",
    "y = credit_data[['Gender','Income_Category']].values\n",
    "imputer_it.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso applichiamo una soluzione piu' 'naive' in cui i valori unknown vengono sostituiti in modo casuale, rispettando la distribuzione dei rimanenti livelli di reddito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ic = credit_data[credit_data['Income_Category'] != 'Unknown']['Income_Category'].value_counts()\n",
    "v, f = count_ic.index, count_ic.values/count_ic.sum()\n",
    "credit_data.loc[credit_data['Income_Category'] == 'Unknown','Income_Category'] = np.random.choice(v,size=1112,p=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['Income_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling categorical data\n",
    "Per ora abbiamo trattato principalmente dati numerici. Tuttavia, dal momento che molti algoritmi di apprendimento non trattano dati categori o testuali, dobbiamo codificare i dati categorici in dati numerici.\n",
    "\n",
    "Un primo approccio per convertire un dato categorico in un dato categorico in un dato numerico e'utilizzare la classe **OrdinalEncoder**, la quale assegna ad ogni categoria un valore numerico distinto.\n",
    "\n",
    "Applichiamo la classe alla colonna 'Card_Category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "credit_data['Card_Category'] = ord_enc.fit_transform(credit_data['Card_Category'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le categorie identificate sono associate all'attributo **categories_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un problema di questo tipo di trasformazione e' l'introduzione di un ordinamento non presente - caso precedente, o il cambiamento dell'ordinamento. \n",
    "\n",
    "Trattiamo il secondo caso, trasformando la colonna categorica'Income_Category'. Nonostante sia una feature categorica, le fasce di reddito sono crescenti quindi esiste un ordinamento. Per rispettare questo ordinamento utilizziamo sempre la classe OrdinalEncoder, ma definiamo a priori la lista ordinata delle categorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_cat = [['Less than $40K','$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +']]\n",
    "ord_enc_income = OrdinalEncoder(categories=rank_cat)\n",
    "credit_data['Income_Category_Num'] = ord_enc_income.fit_transform(credit_data['Income_Category'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data[['Income_Category_Num','Income_Category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per evitare l'introduzione di un ordimento fittizio, si crea una feature binaria per ogni categoria. Per ogni sample, viene posta a 1 la feature/colonna che corrisponde alla categoria originaria. Questo tipo di codifica viene detta **one-hot encoding** ed implementata dalla classe **OneHotEncoder** (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_enc = OneHotEncoder()\n",
    "edu_ohe = edu_enc.fit_transform(credit_data['Education_Level'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['Education_Level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_ohe.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se un attributo di tipo categorico assume valori su un ampio insieme di opzioni, la codifica one-hot encoding restituisce un elevato numero di colonne/dummy features che possono avere un certo impatto sulle performance dell'algoritmo di apprendimento. In questo caso posso a) rappresentare le feature categorico in modo numerico utilizzando un mapping piu' avanzato rispetto alla codifica one-hot; 2) posso ridurre la dimensionalita' della codifica utilizzando/apprendendo un embedding in una spazio a dimensione ridotta rispetto al numero/dimensione della feature categorica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformer\n",
    "E' possibile definire nuovi transfomer in caso ci sia la necessita' di operazioni di data cleaning o combinazione specifiche per il contesto in cui si opera.\n",
    "\n",
    "Al fine di utilizzare il nuovo transformer in una pipeline e dal momento che SKL si basa sul duck typing - \"If it walks like a duck and it quacks like a duck, then it must be a duck\" - devo creare una classe che implementi 3 metodi:\n",
    "- fit\n",
    "- transform\n",
    "- fit_transform\n",
    "\n",
    "Il terzo metodo non lo devo implementare se estendo la classe **TransformerMixin** + se eredito anche dalla classe **BaseEstimator** ottengo anche i metodi **get_params** and **set_params** - utili per il tuning degli iperparametri.\n",
    "\n",
    "Nel nostro esempio proviamo a scrivere un Imputer specifico per la gestione della feature 'Income_Category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnknownImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, nan_value, strategy = 'proportional'):\n",
    "        self.strategy = strategy\n",
    "        self.nan_value = nan_value\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        unique_values, count = np.unique(X,return_counts=True)\n",
    "        self.num_nan = count[unique_values == self.nan_value]\n",
    "        self.count = count[unique_values != self.nan_value]\n",
    "        self.unique_values = unique_values[unique_values != self.nan_value]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        if self.strategy == 'proportional':\n",
    "            freq = self.count / np.sum(self.count)\n",
    "            X_new[X_new == self.nan_value] = np.random.choice(self.unique_values,\n",
    "                             size=self.num_nan,\n",
    "                             p=freq)\n",
    "        if self.strategy == 'uniform':\n",
    "            X_new[X_new == self.nan_value] = np.random.choice(self.unique_values,\n",
    "                                                              size=self.num_nan\n",
    "                                                             )\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso, il transformer ha un iperparametro _strategy_ che puo' essere utilizzato per verificare se la strategia di imputazione influenza le performance degli algoritmi di ML che utilizzeremo in seguito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Una delle trasnformazioni che usualmente si applicano e' la **feature scaling**. Il motivo e' legato alle difficolta' di alcuni algortimi di apprendimento nel gestire dati con ordini di grandezza variabili. Per esempio la distanza euclidea tra due vettori, in cui una dimensione ha ordine di grandezza superiori alla seconda, sara' dominata dalla dimensione con ordine di grandezza superiore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b  = np.array([6, 2500000]) , np.array([14, 2506500])\n",
    "np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un metodo utilizzato per il feature scaling, e' la **standardization**:\n",
    "$$z_i^k=\\frac{x_i^k-\\mu_k}{\\sigma_k}$$\n",
    "dove $\\mu_k$ e $\\sigma_k$ indicano la media e la deviazione standard della feature $k$. La maggior parte dei valori standardizzati appartengono all'intervallo $[-3,3]$, anche se la definizione non mette dei limiti al range di variazione.\n",
    "\n",
    "In SKL il trasformer utilzzato per applicare la standardizzazione e' **StandardScaler**.\n",
    "\n",
    "Per esempio, possiamo applicare la standardizzazione alla colonna 'Age'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "age_scaled = std_scaler.fit_transform(credit_data['Customer_Age'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(age_scaled), max(age_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un metodo alternativo e' riportare i valori di una feature in un intervallo limitato, i.e. con un minimo e un massimo:\n",
    "$$s_i^k=\\frac{x_i^k-min_k}{max_k - min_k}$$\n",
    "dove $min_k$ e $max_k$ indicano rispettivamente il valore minimo e massimo assunti dalla feature $k$. Il vantaggio della normalizzazione e' preservare la sparsita' dei dati. Lo svantaggio e' l'influenza di outlier che aumentano il range tra valore massimo e minimo, tendendo a concentrare la distribuzione dei dati in un sottointervallo di $[0,1]%=$.\n",
    "\n",
    "In SKL la normalizzazione e' implementata dalle classi **MinMaxScaler** e **MaxAbsScaler**. La prima classe normalizza nell'intervallo $[0,1]$, mentre la seconda nell'intervallo $[-1,1]$. La seconda classi si usa con dati gia' centrati rispetto allo zero o dati sparsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "age_minmax = minmax_scaler.fit_transform(credit_data['Customer_Age'].values.reshape(-1,1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se i dati contengono molti outliers oppure la distribuzione e' heavy-tail, uso della standardizzazione puo' non funzionare. In questo caso, la classe **RobustScaler** e' utile in quanto scala la feature utilizzando statistiche robuste agli outlier, i.e. IQR - inter-quantile range e mediana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler = RobustScaler()\n",
    "limit_scaled =robust_scaler.fit_transform(credit_data['Credit_Limit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization\n",
    "Mediante discretizzazione riporto una feature continua in un insieme discreto di valori. La discretizzazione avviene attraverso:\n",
    "1) discretizzazione in k bin\n",
    "2) binarizzazione\n",
    "\n",
    "Il primo metodo e' implementato dalla classe **KBinsDiscretizer**. Il dataset trasforma di defult viene restituito nella codifica one-hot, tuttavia e' possibile cambiare il tipo di codifica, per esempio 'ordinale'. Vengono implementate strategie di binning differenti, selezionabili tramite il parametro _strategy_. Infine posso utilizzare dei bin personalizzati tramite la classe **FunctionTransfomer**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0,18,65,np.inf]\n",
    "labels = ['Giovane', 'Adulto', 'Anziano']\n",
    "age_discretizer = FunctionTransformer(pd.cut,\n",
    "                                     kw_args = {'bins':bins,'labels':labels,'retbins':False})\n",
    "age_discretizer.fit_transform(credit_data['Customer_Age'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il secondo metodo - binarizzazione - trasforma il valore numerico in un valore booleano secondo una soglia. La classe che implementa la binarizzazione e' **Binarizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_binarizer = Binarizer(threshold=18)\n",
    "age_binarizer.fit_transform(credit_data['Customer_Age'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ulteriori approfondimenti sul feature scaling in SKL sono disponibili:\n",
    "- https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
    "- https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipeline\n",
    "La preparazione del dataset per l'utilizzo di un successvio algorimtmo di apprendimento richiede una serie trasformazioni da eseguire nell'rodine corretto.<br>\n",
    "Per facilitare ed organizzare programmaticamente il processo di data preparation, SKL fornisce la classe **Pipeline**.\n",
    "\n",
    "Il costruttore della classe richiede una _lista_ di tuple/coppie (string, oggetto Estimator) che definisco una sequenza di processamento del dataset. Tutti gli oggetti Estimator tranne l'ultimo devono essere Transfomer (metodo fit_transform). Le stringhe definisco dei nomi da assegnare ad ogni step della pipeline e saranno utili nella fase di ottimizzazione degli iperparametri. L'oggetto Pipeline espone i metodi dell'ultima Estimator presente nella lista utilizzata al momento dell'inizializzazione. Il metodo _fit_ della pipeline invoca i metodi *fit_transform* di ogni oggetto Estimator in modo sequenziale, secondo quanto specificato dalla lista.\n",
    "\n",
    "Prima di definire l'intero Pipeline per il preprocessing del dataset, introduciamo la classe **ColumnTransformer** per applicare le trasformazioni specifiche per ogni colonna o sottoinsieme di colonne.\n",
    "\n",
    "A titolo di esempio, decidiamo di applicare una standardizzazione alle colonne di tipo numerico e one-hot encoding alle colonne di tipo categorico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = credit_data.select_dtypes(include=['int64','float64']).columns\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = credit_data.select_dtypes(exclude=['int64','float64']).columns\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = ColumnTransformer([\n",
    "    ('numeric', StandardScaler(), numeric_features),\n",
    "    ('category', OneHotEncoder(), cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trasformed_credit = test_pipeline.fit_transform(credit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trasformed_credit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il costruttore di ColumnTrasformer richiede una lista di tuple di 3 elementi:\n",
    "1) Identificativo - string\n",
    "2) Transformer\n",
    "3) Lista di colonne (nome /o indice) a cui applicare il transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing completo del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_data = pd.read_csv('BankChurnersMissingData.csv')\n",
    "credit_card_data.dropna(subset=['Total_Revolving_Bal','Months_Inactive_12_mon'],\n",
    "                   inplace=True\n",
    "                  )\n",
    "\n",
    "# Estraggo la  colonna delle label e la rimuovo dal dataset\n",
    "credit_card_label = credit_card_data['Attrition_Flag'].map(\n",
    "    {'Existing Customer':0,\n",
    "     'Attrited Customer':1\n",
    "    }\n",
    ")\n",
    "credit_card_data.drop(columns=['Attrition_Flag',\n",
    "                               'CLIENTNUM',\n",
    "                               'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n",
    "                               'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1'],\n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo un Imputer personalizzato, utilizzando la classe **FunctionTransformer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_imputer(X, missing_value = 'Unknown'):\n",
    "    X = X.values\n",
    "    unique_values, count = np.unique(X,return_counts=True)\n",
    "    num_nan = count[unique_values == missing_value]\n",
    "    counting = count[unique_values != missing_value]\n",
    "    values = unique_values[unique_values != missing_value]\n",
    "    X_new = X.copy()\n",
    "    freq = counting / np.sum(counting)\n",
    "    X_new[X_new == missing_value] = np.random.choice(values,size=num_nan,p=freq)\n",
    "    return X_new\n",
    "\n",
    "ui = FunctionTransformer(unknown_imputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definisco un insieme di Pipeline per gestire i dati mancanti di Customer_Age e le etichette 'Unknown' che caratterizzano Education_Level, Marital_Status, Income_Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_age_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "category_pipeline = Pipeline([\n",
    "    ('imputer', FunctionTransformer(unknown_imputer)),\n",
    "    ('ordinal', OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per alcune feature numeriche utilizzo uno scaler RobustScaler, dal momento sembrano di tipo heavy-tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_robust = ['Credit_Limit','Total_Revolving_Bal','Avg_Open_To_Buy']\n",
    "features_standard = list(set(credit_card_data.select_dtypes(include=['int64','float64']).columns).difference(set(features_robust + ['Avg_Utilization_Ratio', 'Customer_Age'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine definisco la mia pipeline di data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing = ColumnTransformer([\n",
    "    ('age', customer_age_pipeline, ['Customer_Age']),\n",
    "    ('gender', OrdinalEncoder(categories=[['M','F']]), ['Gender']),\n",
    "    ('edu', category_pipeline, ['Education_Level']),\n",
    "    ('status', category_pipeline, ['Marital_Status']),\n",
    "    ('income', category_pipeline, ['Income_Category']),\n",
    "    ('card', category_pipeline, ['Card_Category']),\n",
    "    ('numeric_robust', RobustScaler(), features_robust),\n",
    "    ('feature_standard', StandardScaler(), features_standard)\n",
    "],\n",
    "    remainder = 'passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo la feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = data_preprocessing.fit_transform(credit_card_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... e per il futuro anche la sequenza delle colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['Customer_Age','Gender']\n",
    "for c in ['Education_Level','Marital_Status','Income_Category']:\n",
    "    cat_inc_name = [c+'_cat{}'.format(i) for i in range(1,len(credit_card_data[c].unique()))]\n",
    "    columns_name.extend(cat_inc_name)\n",
    "columns_name.extend(['Card_Category_cat{}'.format(i) for i in range(1,len(credit_card_data['Card_Category'].unique())+1)])\n",
    "columns_name.extend(features_robust)\n",
    "columns_name.extend(features_standard)\n",
    "columns_name.append('Avg_Utilization_Ratio')\n",
    "columns_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing with Tensor Flow\n",
    "\n",
    "Un ulteriore strumento per la trasformazione del dato ci viene fornito da TensorFlow. TF facilita la gestione di dataset di grandi dimensioni e la relativa fase di pre-processing mediante le **Data API** che astraggono tutte le operazioni di basso livello come il multithreading, la gestione delle code di elaborazione e il precaricamento del dato in memoria principale - RAM.\n",
    "\n",
    "Mediante Data API e' possibile leggere/scrivere file testuali, file binari con dimensione fissa/variabile dei record oppure accedere a database SQL. Oltre alla fase di caricmaneto del dato, Data API permette la gestione e trasformazioni di feature numeriche, categoriche, testuali mediante diverse modalita' di codifica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data API\n",
    "Il concetto primario delle Data API e'  il **dataset** = sequenza di elementi dato. All'oggetto dataset e' possibile applicare una serie di trasformazioni come in SKL mediante un insieme di metodi di trasformazioni; ognuno dei quali restituisce un nuovo oggetto dataset.\n",
    "\n",
    "Nell'esempio creiamo un dataset di 10 elementi, dove ogni elemento e' un tensore 0D, ovvero uno scalare. Da questo dataset, ne creiamo uno nuovo che contiene la ripetizione del primo - 3 volte. Infine, definiamo un nuovo dataset in cui gli elementi vengono raggruppati in gruppi - **batch** - di 7 elementi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ripetizione del dataset avviene mediante il metodo **repeat**, senza che venga eseguita un'effettiva copia del dataset in memoria principale. Il raggruppamento e' eseguito dal metodo **batch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' possibile applicare una funzione a ciascun elemento del dataset mediante il metodo **map**, eventualmente in modo \"parallelo\" specificando il parametro *num_parallel_calls*.\n",
    "\n",
    "Una trasformazione e' applicabile all'interno dataset mediante il metodo **apply**.\n",
    "\n",
    "Infine mediante il metodo **filter** rimuoviamo gli item che non soddisfano una condizione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x*2).apply(tf.data.experimental.unbatch()).filter(lambda y: (y % 6) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling data\n",
    "Abbiamo gia' visto come mescolare il training set rafforzi le assunzioni i.i.d. dei sample del dataset. In TF il metodo piu' semplice per eseguire lo shuffling e' attraverso il metodo **shuffle**. Esso crea un nuovo dataset e inizialmente riempie un buffer con i primi elementi del dataset originale. Quando viene richiesto un elemento, esso viene estratto in modo casuale dal buffer e un nuovo elemento dal dataset originale viene spostato nel buffer. La dimensione del buffer puo' essere specificata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset:\n",
    "    print(i.numpy(),end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shuffled = dataset.shuffle(buffer_size=4, seed=42)\n",
    "for i in dataset_shuffled:\n",
    "    print(i.numpy(), end=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per dataset che non possono caricati completamente in memoria principale, il precedente metodo e' poco efficace. Un'alternativa e' dividere il file del datasets in file di dimensioni ridotte e leggerli simultaneamente interponendo i diversi flussi di lettura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = ['house_dataset\\\\train{:02d}.csv'.format(i) for i in range(20)] \n",
    "valid_filepaths = ['house_dataset\\\\validation{:02d}.csv'.format(i) for i in range(10)]\n",
    "test_filepaths = ['house_dataset\\\\test{:02d}.csv'.format(i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di default il precedente metodo restituisce un dataset che mescola i percorsi dei file.\n",
    "\n",
    "Ora, mediante il metodo **interleave** leggiamo simultaneamente da 5 file reader e impiliamo le linee provenienti dai diversi reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda fp: tf.data.TextLineDataset(fp).skip(1),\n",
    "    cycle_length=n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo parametro specifica la funzione da applicare ad ogni percorso del file. Per leggere i file in \"parallelo\" specifico il parametro *num_parallel_calls*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come ultimo passo dobbiamo convertire ogni item - ora e' una stringa binaria - in un tensore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo **tf.io.decode_csv** restituisce una lista di 0D Tensor - scalare - i cui valori di default sono specificati dal parametro *record_dafeaults*. Per restituire un tensore 1D trasformiamo la lista in un vettore mediante il metodo **stack**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riassumendo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat).interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers,\n",
    "        num_parallel_calls=n_read_threads\n",
    "    ).shuffle(shuffle_buffer_size).map(preprocess, \n",
    "                                       num_parallel_calls=n_parse_threads\n",
    "                                      ).batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante il metodo **prefecth** indichiamo che durante la fase di training sul batch $n$, venga caricato in memoria il batch $n+1$, in modo da mantenere il carico computazionale attivo e costante, specialmente con architetture CPU + GPU.\n",
    "\n",
    "![](prefetch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo utilizzare la precedente funzione per creare un oggetto dataset e utilizzare Keras per effettuare il training di un MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "history = model.fit(train_set,\n",
    "          steps_per_epoch=len(X_train) // batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_set.map(lambda X, y: X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord\n",
    "TFRecord e' il formato utilizzato da TF per memorizzare e leggere una grande quantita' di dati. E' un formato binario che contiene una **sequenza** di record binari di lunghezza variabile. \n",
    "\n",
    "E' possibile creare un TFRecord mediante la classe **TFRecordwriter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"test.tfrecord\") as f:\n",
    "    f.write(b\"Primo record\")\n",
    "    f.write(b\"Secondo record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentre posso leggere/caricare un TFRecord mediante la classe **TFRecordDataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percorso = [\"test.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(percorso)\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFRecordDataset puo' leggere piu' file contemporaneamente specificando il parametro *num_parallel_reads*.\n",
    "\n",
    "Inoltre, specificando il parametro *compression_type* si puo' comprimere il TFRecord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"test_compresso.tfrecord\", options ) as f:\n",
    "    f.write(b\"Primo record\")\n",
    "    f.write(b\"Secondo record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tf.data.TFRecordDataset([\"test_compresso.tfrecord\"], compression_type=\"GZIP\"):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFRecord contengono elementi serializzati di tipo **Protocol Buffer - PB**. PB e' formato binario sviluppato da Google il cui schema e' definibile secondo un linguaggio specifico. Per esempio\n",
    "```\n",
    "syntax = \"proto3\";\n",
    "message Person {\n",
    "  string name = 1;\n",
    "  int32 id = 2;\n",
    "  repeated string email = 3;\n",
    "}\n",
    "```\n",
    "Definisce un PB di tipo Person con un campo stringa di nome 'name' e indice 1, un campo intero id all'indice 2 ed una lista di stringhe il cui nome e' 'email' e il cui indice e' 3. Il valore repeated specifica che il campo email puo' contenere da 0 a molte stringhe.\n",
    "\n",
    "In TF il PB principale e' **Example** che definisce lo schema di un'istanza che compone un dataset. Nello specifico la sua definzione e' la seguente:\n",
    "```\n",
    "syntax = \"proto3\";\n",
    "\n",
    "message BytesList { repeated bytes value = 1; }\n",
    "message FloatList { repeated float value = 1 [packed = true]; }\n",
    "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
    "message Feature {\n",
    "    oneof kind {\n",
    "        BytesList bytes_list = 1;\n",
    "        FloatList float_list = 2;\n",
    "        Int64List int64_list = 3;\n",
    "    }\n",
    "};\n",
    "message Features { map<string, Feature> feature = 1; };\n",
    "message Example { Features features = 1; };\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contatto1 = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "        }))\n",
    "\n",
    "with tf.io.TFRecordWriter(\"contatto.tfrecord\") as f:\n",
    "    f.write(contatto1.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso la definizine di un record e' piuttosto dettagliata. Una volta definito l'oggetto Example lo serializziamo mediante il metodo **SerializeToString** e analogamente a quanto fatto in precedenza lo possiamo scrivere in un file TFRecord."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per caricare il PB Example, applichiamo il metodo **parse_single_example** ad ogni PB Example che otteniamo mediante la classe TFRecordDataset. Il metodo richiede la stringa binaria contenente il PB serializzato e una descrizione di ogni campo del PB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "for serialized_example in tf.data.TFRecordDataset([\"contatto.tfrecord\"]):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
    "                                                feature_description)\n",
    "parsed_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una BytesList puo' contenere un qualsiasi dato binario. Proviamo con un'immagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "img = load_sample_images()[\"images\"][0]\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Immagine originaria\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo serializzare l'immagine utilizzando la codifica JPEG mediante il metodo **encode_jpeg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.io.encode_jpeg(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E definire un nuovo PB Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_immagine = Example(features=Features(\n",
    "    feature={\n",
    "        \"image\": Feature(bytes_list=BytesList(value=[data.numpy()]))\n",
    "    }\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Che posso serializzare e scrivere in un TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"immagini.tfrecord\") as f:\n",
    "    f.write(example_immagine.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"immagini_collection.tfrecord\") as f:\n",
    "    for img in load_sample_images()['images']:\n",
    "        f.write(Example(features=\n",
    "                        Features(feature={\n",
    "                            \"image\": Feature(bytes_list=BytesList(value=[tf.io.encode_jpeg(img).numpy()]))\n",
    "                        }\n",
    "                                )\n",
    "                       ).SerializeToString()\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalla collection definita in precedenza posso ripristinare le immagini nel seguente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = { \"image\": tf.io.VarLenFeature(tf.string) }\n",
    "list_image = [tf.io.decode_jpeg(tf.io.parse_single_example(example,\n",
    "                                                           feature_description)['image'].values[0])\n",
    "              for example in tf.data.TFRecordDataset([\"immagini_collection.tfrecord\"])\n",
    "             ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(list_image[1])\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Immagine originaria\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il PB Example non e' adatto a descrivere tipi di dati in cui si hanno sequenze di sequenze - list of lists -, ad esempio nel caso di classificazione di documenti testuali, dove un documento e' una sequenza di frasi ed ogni frase e' una sequena di parole. Per tali situazioni si utilizza il PB **SequenceExample**, la cui definizione e':\n",
    "```\n",
    "syntax = \"proto3\";\n",
    "\n",
    "message FeatureList { repeated Feature feature = 1; };\n",
    "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
    "message SequenceExample {\n",
    "  Features context = 1;\n",
    "  FeatureLists feature_lists = 2;\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import FeatureList, FeatureLists, SequenceExample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come per il PB Example, per SequenceExample abbiamo un metodo **parse_single_sequence_example** per ricostruire il PB SequenceExample serializzato. \n",
    "\n",
    "Nell'esempio inseriamo uno stralcio di un articolo di una testata giornalistica online in un TFRecord utilizzando il PB SequenceExample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = Features(feature={\n",
    "    \"author_name\": Feature(bytes_list=BytesList(value=[b'Il Post'])),\n",
    "    \"title\": Feature(bytes_list=BytesList(value=[word.encode('utf-8') for word in 'Il piÃ¹ grande mercato azionario dâ€™Europa non Ã¨ piÃ¹ Londra, ma Amsterdam'.split()])),\n",
    "    \"pub_date\": Feature(int64_list=Int64List(value=[2021, 2, 12]))\n",
    "})\n",
    "\n",
    "document = \"\"\"A gennaio Amsterdam Ã¨ diventata il piÃ¹ grande centro di scambio di azioni dâ€™Europa, aumentando di circa quattro volte il valore di quelle scambiate quotidianamente rispetto al 2020 e superando Londra, che per decenni Ã¨ stata il piÃ¹ grande e importante mercato finanziario europeo.\n",
    "Come ha raccontato il Financial Times, questo importante cambiamento Ã¨ stato provocato da Brexit: a causa di un mancato accordo tra Regno Unito e Unione Europea, a partire da gennaio, quando si Ã¨ verificata completamente lâ€™uscita dallâ€™Unione, sui mercati finanziari britannici non Ã¨ piÃ¹ possibile comprare e vendere titoli denominati in euro: questo ha provocato un enorme spostamento di transazioni da un mercato finanziario a un altro.\"\"\"\n",
    "\n",
    "content_features = [Feature(bytes_list=BytesList(value=[word.encode('utf-8') for word in sentence.split()])) \n",
    "                    for sentence in document.split('\\n')\n",
    "                   ]\n",
    "\n",
    "sequence_example = SequenceExample(\n",
    "    context = context,\n",
    "    feature_lists=FeatureLists(\n",
    "        feature_list={\n",
    "            \"content\": FeatureList(feature=content_features)\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_example_serializzato = sequence_example.SerializeToString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applico il metodo di parsing o de-serializzazione del PB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_feature_descriptions = {\n",
    "    \"author_name\": tf.io.VarLenFeature(tf.string),\n",
    "    \"title\": tf.io.VarLenFeature(tf.string),\n",
    "    \"pub_date\": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]),\n",
    "}\n",
    "sequence_feature_descriptions = {\n",
    "    \"content\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
    "    sequence_example_serializzato,\n",
    "    context_feature_descriptions,\n",
    "    sequence_feature_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.sparse.to_dense(\n",
    "    parsed_feature_lists[\"content\"]\n",
    ")[0])\n",
    "print(tf.sparse.to_dense(\n",
    "    parsed_feature_lists[\"content\"]\n",
    ")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full),(X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Layer in TF\n",
    "Anche TF permette di pre-processare i dati in modo da trasformare ogni feature in feature numerica. Nello specifico vengono definiti una serie di preprocessing layer, alcuni dei quali sono 'addestrabili'.\n",
    "\n",
    "Un primo approccio e' implementare il layer di preprocessing mediante un layer custom  estendendo la classe **Layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization = Standardization(input_shape=[28, 28])\n",
    "\n",
    "sample_images = X_train[:2,:,:]\n",
    "sample_images.shape\n",
    "standardization.adapt(sample_images)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    standardization,\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "          epochs=5,\n",
    "          validation_data=(X_valid,y_valid)\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature categoriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house = pd.read_csv('housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = house['median_house_value'].values\n",
    "X = house.drop(['median_house_value','total_bedrooms'] , axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_train_cat = X_train[:,:-1].astype('float64'), X_train[:,-1]\n",
    "X_test_num, X_test_cat = X_test[:,:-1].astype('float64'), X_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per la feature categorica  dobbiamo assegnare ad ogni categoria un indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.unique(X_train_cat)\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo un initializzatore per definire una tabella di lookup, passando il vocabolario e gli indici da associare agli elementi del vocabolario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per la creazione della tabella di lookup - agisce come un dict in Python - inseriamo dei bucket vuoti che verranno riempiti quando si cercano degli elementi non presenti nel vocabolario iniziale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_categories = tf.constant(['NEAR BAY','DESERT'])\n",
    "cat_indices = table.lookup(test_categories)\n",
    "print(cat_indices)\n",
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "cat_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_layer = keras.layers.experimental.preprocessing.TextVectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_layer.adapt(X_train_cat[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_layer(X_train_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel caso in cui le categorie per feature siano maggiori di 10 - indicativamente - e' possibile mappare ogni categoria in un spazio a bassa dimensionalita' - con dimensione minore del numero di categorie. Questo processo viene definito come **embedding** e consiste nell'assegnare ad ogni valore categorico un vettore di dimensioni ridotte.\n",
    "\n",
    "In Keras possiamo definire un layer **EmbeddingMatrix** che rappresenta un mapping tra i valori della feature categorica e vettori nello spazio a bassa dimensionalita'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "embedding = keras.layers.Embedding(input_dim = len(vocab) + num_oov_buckets, output_dim = embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal momento che l'embedding e' adddestrabile possiamo imparare il mapping migliore modificando i pesi della matrice precedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization = Standardization()\n",
    "sample_ = X_train_num[:100]\n",
    "standardization.adapt(sample_)\n",
    "\n",
    "num_input = keras.layers.Input(shape=X_train_num.shape[1:])\n",
    "cat_input = keras.layers.Input([], dtype=tf.string)\n",
    "standard_input = standardization(num_input)\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(cat_input)\n",
    "cat_embedding = keras.layers.Embedding(input_dim = len(vocab) + num_oov_buckets, output_dim=2, name = 'embedding')(cat_indices)\n",
    "encoded_input = keras.layers.concatenate([standard_input, cat_embedding])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(encoded_input)\n",
    "hidden2 = keras.layers.Dense(10, activation=\"relu\")(hidden1)\n",
    "output = keras.layers.Dense(1)(hidden2)\n",
    "model = keras.models.Model(inputs=[num_input, cat_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3)\n",
    "             )\n",
    "history = model.fit((X_train_num,X_train_cat), y_train,\n",
    "                    epochs=20\n",
    "                   )\n",
    "mse_test = model.evaluate((X_test_num, X_test_cat), y_test)\n",
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot()\n",
    "for i, e in enumerate(vocab):\n",
    "    x,y = model.layers[4].weights[0][i].numpy()\n",
    "    ax.scatter(x,y)\n",
    "    ax.text(x,y, e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
